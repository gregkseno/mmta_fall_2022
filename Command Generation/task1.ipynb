{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YyDL6qIQHeXQ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Практическое задание 1\n",
    "\n",
    "# Ранжирование вопросов StackOverflow с помощью векторных представлений слов\n",
    "\n",
    "## курс \"Математические методы анализа текстов\"\n",
    "\n",
    "\n",
    "### ФИО: Ксенофонтов Григорий Сергеевич"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6qAip5K2HeXU",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Введение\n",
    "\n",
    "В этом задании вы научитесь вычислять близость текстов и применить этот метод для поиска похожих вопросов на [StackOverflow](https://stackoverflow.com).\n",
    "\n",
    "### Используемые библиотеки\n",
    "\n",
    "В данном задании потребуются следующие библиотеки:\n",
    "- [Gensim](https://radimrehurek.com/gensim/) — инструмент для решения различных задач NLP (тематическое моделирование, представление текстов, ...).\n",
    "- [Numpy](http://www.numpy.org) — библиотека для научных вычислений.\n",
    "- [scikit-learn](http://scikit-learn.org/stable/index.html) — библилиотека с многими реализованными алгоритмами машинного обучения для анализа данных.\n",
    "- [Nltk](http://www.nltk.org) — инструмент для работы с естественными языками.\n",
    "- [Pytorch](https://pytorch.org/) — инструмент для обучения нейросетей.\n",
    "\n",
    "\n",
    "### Данные\n",
    "\n",
    "Данные лежат в архиве `StackOverflowData.zip`, который состоит из:\n",
    "- `train.tsv` - обучающая выборка. В каждой строке через табуляцию записаны дублирующие друг друга предложения;\n",
    "- `test.tsv` - тестовая выборка. В каждой строке через табуляцию записаны: *<вопрос>, <похожий вопрос>, <отрицательный пример 1>, <отрицательный пример 2>, ...*\n",
    "\n",
    "Скачать архив можно здесь: [ссылка на google диск](https://drive.google.com/open?id=1QqT4D0EoqJTy7v9VrNCYD-m964XZFR7_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bmi5w7aLHeXV",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Тесты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SKJbaNqbH6DE",
    "outputId": "3610d657-1005-4e93-e22d-29b009b14e97",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1666036414185,
     "user_tz": -180,
     "elapsed": 221697,
     "user": {
      "displayName": "Григорий Ксенофонтов",
      "userId": "05861251971421218806"
     }
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aTFSi6ApObpz",
    "outputId": "e4247ce8-c147-4972-ae8e-3d51cd7d8cb5",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1666036415319,
     "user_tz": -180,
     "elapsed": 1147,
     "user": {
      "displayName": "Григорий Ксенофонтов",
      "userId": "05861251971421218806"
     }
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "data\t\t\t\t       __pycache__\n",
      "download_utils.py\t\t       stackoverflow_similar_questions.zip\n",
      "GoogleNews-vectors-negative300.bin.gz  task1.ipynb\n",
      "mmta_tests.py\t\t\t       test_gt.json\n"
     ]
    }
   ],
   "source": [
    "!ls /content/drive/\"MyDrive\"/\"Colab Notebooks\"/\"MMTA FALL 2022\"/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "XEgz7_DmOMsX",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1666036415319,
     "user_tz": -180,
     "elapsed": 3,
     "user": {
      "displayName": "Григорий Ксенофонтов",
      "userId": "05861251971421218806"
     }
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/content/drive/MyDrive/Colab Notebooks/MMTA FALL 2022')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "AFddHZb1HeXV",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1666036418178,
     "user_tz": -180,
     "elapsed": 2861,
     "user": {
      "displayName": "Григорий Ксенофонтов",
      "userId": "05861251971421218806"
     }
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from mmta_tests import TaskTests\n",
    "task_tests = TaskTests.from_json(path='/content/drive/MyDrive/Colab Notebooks/MMTA FALL 2022/test_gt.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WoCiEkkDHeXX",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Вектора слов\n",
    "\n",
    "Для решения вам потребуются предобученная модель векторных представлений слов. Используйте [модель эмбеддингов](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit), которая была обучена с помощью пакета word2vec на данных Google News (100 миллиардов слов). Модель содержит 300-мерные вектора для 3 миллионов слов и фраз. Вы можете скачать их, запустив блок кода ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "pEdCO2F-HeXX",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1666036418179,
     "user_tz": -180,
     "elapsed": 13,
     "user": {
      "displayName": "Григорий Ксенофонтов",
      "userId": "05861251971421218806"
     }
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# from download_utils import download_google_vectors\n",
    "\n",
    "\n",
    "#download_google_vectors(target_dir='/content/drive/MyDrive/Colab Notebooks/MMTA FALL 2022/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cUHUsLGZHeXY",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Часть 1. Предобученные векторные представления слов (2 балла)\n",
    "\n",
    "Скачайте предобученные вектора и загрузите их с помощью функции [KeyedVectors.load_word2vec_format](https://radimrehurek.com/gensim/models/keyedvectors.html) библиотеки Gensim с параметром *binary=True*. Если суммарный размер векторов больше, чем доступная память, то вы можете загрузите только часть векторов, указав параметр *limit* (рекомендуемое значение: 500000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "UnvdW80yHeXZ",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1666036437127,
     "user_tz": -180,
     "elapsed": 18961,
     "user": {
      "displayName": "Григорий Ксенофонтов",
      "userId": "05861251971421218806"
     }
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "\n",
    "wv_embeddings = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "    '/content/drive/MyDrive/Colab Notebooks/MMTA FALL 2022/GoogleNews-vectors-negative300.bin.gz', binary=True, limit=500000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZYfovnPSHeXZ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Как пользоваться этими векторами?\n",
    "\n",
    "Как только вы загрузите векторные представления слов в память, убедитесь, что имеете к ним доступ. Сначала вы можете проверить, содержится ли какое-то слово в загруженных эмбедингах:\n",
    "\n",
    "    'word' in wv_embeddings\n",
    "\n",
    "Затем, чтобы получить соответствующий вектор, вы можете использовать оператор доступа по ключу:\n",
    "\n",
    "    wv_embeddings['word']\n",
    "\n",
    "### Проверим, корректны ли векторные представления\n",
    "\n",
    "Чтобы предотвратить возможные ошибки во время первого этапа, можно проверить, что загруженные вектора корректны. Для этого проверьте три пункта:\n",
    "1. Используя метод `.most_similar(positive=..., negative=...)`, найти слово, похожее на `woman`, `king` и непохожее на `man`.\n",
    "2. Используя метод `.doesnt_match(...)`, найти \"белую ворону\" в списке `['breakfast, 'dinner', 'lunch', 'cereal']`.\n",
    "3. Используя метод `.most_similar_to_given(word, [...])`, найти наиболее похожее на `music` слово из списка `['water', 'sound', 'backpack', 'mouse']`.\n",
    "\n",
    "Прокомментируйте полученные результаты: считаете ли вы их верными и почему."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sc5fFIN7HeXa",
    "outputId": "31264021-c930-42b9-8d53-eeebb8d0105d",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1666036437526,
     "user_tz": -180,
     "elapsed": 411,
     "user": {
      "displayName": "Григорий Ксенофонтов",
      "userId": "05861251971421218806"
     }
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('queen', 0.7118192911148071), ('monarch', 0.6189674139022827), ('princess', 0.5902431011199951), ('crown_prince', 0.5499460697174072), ('prince', 0.5377321243286133), ('kings', 0.5236844420433044), ('queens', 0.518113374710083), ('sultan', 0.5098593235015869), ('monarchy', 0.5087411999702454), ('royal_palace', 0.5087165832519531)]\n",
      "cereal\n",
      "sound\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/gensim/models/keyedvectors.py:895: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n"
     ]
    }
   ],
   "source": [
    "###########################\n",
    "### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "###########################\n",
    "print(wv_embeddings.most_similar(positive=['king', 'woman'], negative=['man']))\n",
    "print(wv_embeddings.doesnt_match(['breakfast', 'dinner', 'lunch', 'cereal']))\n",
    "print(wv_embeddings.most_similar_to_given('music', ['water', 'sound', 'backpack', 'mouse']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_VNLgjTDHeXb",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Ранжирование вопросов StackOverflow\n",
    "\n",
    "Давайте посмотрим на данные, которые мы будем использовать в рамках задания. Выборка уже разбита на обучающую и тестовую. Все файлы используют табуляцию в качестве разделителя, но они имеют разный формат:\n",
    "\n",
    "- *обучающая* выборка (train.tsv) содержит похожие друг на друга предложения в каждой строке;\n",
    "- *тестовая* выборка (validation.tsv) содержит в каждой строке: *вопрос, похожий вопрос, отрицательный пример 1, отрицательный пример 2, ...*\n",
    "\n",
    "Считайте тестовую (валидационную) выборку. Ответьте на следующие вопросы:\n",
    "1. Сколько пар-дубликатов предоставлено в выборке?\n",
    "2. Сколько в среднем на каждую пару предоставлено отрицательных примеров?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "3ynHipc7HeXb",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1666036440133,
     "user_tz": -180,
     "elapsed": 2609,
     "user": {
      "displayName": "Григорий Ксенофонтов",
      "userId": "05861251971421218806"
     }
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "\n",
    "def read_corpus(filename):\n",
    "    data = []\n",
    "    for line in open(filename, encoding='utf-8'):\n",
    "        data.append(line.strip().split('\\t'))\n",
    "    return data\n",
    "\n",
    "validation = read_corpus('/content/drive/MyDrive/Colab Notebooks/MMTA FALL 2022/data/validation.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "kUgugLHrHeXc",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1666036440134,
     "user_tz": -180,
     "elapsed": 7,
     "user": {
      "displayName": "Григорий Ксенофонтов",
      "userId": "05861251971421218806"
     }
    },
    "outputId": "891b1d85-9681-4cb2-e79c-3ac7e1386640",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "3760\n",
      "998.8106382978723\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "###########################\n",
    "### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "###########################\n",
    "\n",
    "num_samples = len(validation) # Сколько пар-дубликатов предоставлено в выборке?\n",
    "print(num_samples)\n",
    "\n",
    "amount_of_negatives_per_sample = np.mean(list(map(lambda x: len(x)-2, validation))) # Сколько в среднем на каждую пару предоставлено отрицательных примеров?\n",
    "print(amount_of_negatives_per_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "tagPJn0lHeXc",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1666036440134,
     "user_tz": -180,
     "elapsed": 5,
     "user": {
      "displayName": "Григорий Ксенофонтов",
      "userId": "05861251971421218806"
     }
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "task_tests.test_validation_corpus(\n",
    "    num_samples,\n",
    "    amount_of_negatives_per_sample\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mZl2Fz4CHeXd",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Векторные представления текста\n",
    "\n",
    "Чтобы перейти от отдельных слов к векторным представлениям вопросов, предлагается подсчитать **среднее** векторов всех слов в вопросе. Если для какого-то слова нет предобученного вектоора, то его нужно пропустить. Если вопрос не содержит ни одного известного слова, то нужно вернуть нулевой вектор.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "puwm3hA1HeXd",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1666036440135,
     "user_tz": -180,
     "elapsed": 6,
     "user": {
      "displayName": "Григорий Ксенофонтов",
      "userId": "05861251971421218806"
     }
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Embedder:\n",
    "    \n",
    "    def __init__(self, embeddings, dim):\n",
    "        \"\"\"\n",
    "            embeddings: word2vec эмбеддинги\n",
    "            dim: размерность word2vec эмбеддингов. Нужна для задания нулего вектора для пустых вопросов\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        self.embeddings = embeddings\n",
    "        self.dim = dim\n",
    "        \n",
    "    def __call__(self, text, normalize=False):\n",
    "        \"\"\"\n",
    "            Принимает на вход текст и преобразует его в вектор.\n",
    "            \n",
    "            text: строка с вопросом\n",
    "            normalize: при True нужно перед возвращением нормализовать вектор\n",
    "            \n",
    "            returns: вектор вопроса\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        cosine = np.zeros(self.dim)\n",
    "        sentence = text.replace(',', ' ').replace('.', ' ').replace('?', ' ').split(' ')\n",
    "        sentence = list(filter(lambda el: el != '', sentence))\n",
    "        n = len(sentence)\n",
    "\n",
    "        \n",
    "        for word in sentence:\n",
    "          try:\n",
    "            cosine += self.embeddings[word]\n",
    "          except KeyError:\n",
    "            n -= 1\n",
    "\n",
    "        res = cosine / n\n",
    "        if normalize:\n",
    "          return  res / np.linalg.norm(res)\n",
    "        else: \n",
    "          return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "4TlgTYNPHeXd",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1666036440135,
     "user_tz": -180,
     "elapsed": 6,
     "user": {
      "displayName": "Григорий Ксенофонтов",
      "userId": "05861251971421218806"
     }
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "embedder = Embedder(wv_embeddings, dim=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "SvVJ-APXHeXe",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1666036440135,
     "user_tz": -180,
     "elapsed": 5,
     "user": {
      "displayName": "Григорий Ксенофонтов",
      "userId": "05861251971421218806"
     }
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#task_tests.test_embedder(embedder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NV-BANNDHeXe",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Теперь у нас есть метод для создания векторного представления любого предложения. Оценим, как будет работать это решение.\n",
    "\n",
    "### Оценка близости текстов\n",
    "\n",
    "В качестве метрики схожести вопросов будем использовать косинусную близость.\n",
    "\n",
    "В валидационном датасете для каждой пары вопросов-дубликатов у нас есть случайные отрицательные примеры. Для каждого триплета (вопрос, дубликат, отрицательные примеры) будет ранжировать с помощью нашей модели и косинусной близости дубликат и отрицательные примеры и смотреть на позицию дубликата.\n",
    "\n",
    "#### Hits@K\n",
    "Довольно простой и легко интерпретируемой метрикой будет количество корректных попаданий дубликата в top \"выдачи\" для какого-то *K*:\n",
    "$$ \\text{Hits@K} = \\frac{1}{N}\\sum_{i=1}^N \\, [dup_i \\in topK(q_i)],$$\n",
    "где $q_i$ - $i$-ый вопрос, $dup_i$ - его дубликат, $topK(q_i)$ - первые *K* элементов в ранжированном списке, который выдает наша модель.\n",
    "\n",
    "#### Пример оценок\n",
    "\n",
    "Пусть $N = 1$, вопрос $q_1$ это \"Что такое python\", а его дубликат $dup_1$ это \"Что такое язык python\". Пусть модель выдала следующий ранжированный список кандидатов:\n",
    "\n",
    "1. *\"Как узнать с++\"*\n",
    "2. *\"Что такое язык python\"*\n",
    "3. *\"Хочу учить Java\"*\n",
    "4. *\"Не понимаю Tensorflow\"*\n",
    "\n",
    "Вычислим метрику *Hits@K* для *K = 1, 4*:\n",
    "\n",
    "- [K = 1] $\\text{Hits@1} =  [dup_1 \\in top1(q_1)] = 0$\n",
    "- [K = 4] $\\text{Hits@4} =  [dup_1 \\in top4(q_1)] = 1$\n",
    "\n",
    "#### Подсчет метрики Hits@k сразу для нескольких k\n",
    "\n",
    "Чтобы посчитать метрику для нескольких k, не нужно повторно ранжировать нашей моделью вопросы для одного и того же сэмпла. Достаточно посчитать для сэмпла количество **сложных негативов** - отрицательных примеров, оказавшихся в выдаче выше, чем дубликат. Тогда\n",
    "$$Hits@k = \\begin{cases}\n",
    "    1, & N < k \\\\\n",
    "    0, & иначе\n",
    "   \\end{cases},$$\n",
    "где **N** - количество сложных негативов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uKm6-E7-HeXf",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Реализуйте подсчет Hits@k для произвольного набора значений k и заданной валидационной выборки, используя предложенный шаблон."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "UTBO3BLmHeXf",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1666036440615,
     "user_tz": -180,
     "elapsed": 485,
     "user": {
      "displayName": "Григорий Ксенофонтов",
      "userId": "05861251971421218806"
     }
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "np.seterr(divide='ignore', invalid='ignore')        \n",
    "    \n",
    "class Scorer:\n",
    "    \n",
    "    def __init__(self, k, embedder):\n",
    "        \"\"\"\n",
    "            k: список значений k, для которых нужно посчитать hits@k\n",
    "            embedder: объект класса Embedder, умеющий преобразовать текст в вектор\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        self.k = k\n",
    "        self.embedder = embedder\n",
    "        \n",
    "    def _get_hard_negatives(self, q, pos, negs):\n",
    "        \"\"\"\n",
    "            q: текст вопроса\n",
    "            pos: текст дубликата\n",
    "            negs: список из текстов случайных вопросов\n",
    "            \n",
    "            result: количество сложных отрицательных примеров, оказавшихся выше положительного\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        q_vec = self.embedder(q)\n",
    "        q_vec = np.nan_to_num(q_vec, nan=0.0).reshape(1, -1)\n",
    "        pos_vec = self.embedder(pos)\n",
    "        pos_vec = np.nan_to_num(pos_vec, nan=0.0).reshape(1, -1)\n",
    "        negs_vecs = np.array(list(map(self.embedder, negs)))\n",
    "        negs_vecs = np.nan_to_num(negs_vecs, nan=0.0)\n",
    "\n",
    "        pos_similarity = cosine_similarity(pos_vec, q_vec).item()\n",
    "        negs_similarity = cosine_similarity(negs_vecs, q_vec).reshape(-1)\n",
    "        return np.sum(negs_similarity > pos_similarity)\n",
    "\n",
    "\n",
    "    \n",
    "    def __call__(self, samples, verbose=False):\n",
    "        \"\"\"\n",
    "            samples: список из списков вида [q, pos, neg1, neg2, ...]. Наша валидационная выборка\n",
    "            verbose: выводить progressbar подсчета метрики с помощью tqdm\n",
    "            \n",
    "            result: словарь вида {k: hits@k}\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        hits = {k: 0 for k in self.k}\n",
    "        range_ = tqdm.tqdm(samples) if verbose else samples\n",
    "        for sample in range_:\n",
    "          N = self._get_hard_negatives(sample[0], sample[1], sample[2:])\n",
    "          hits = {hit[0]: hit[1] + 1 if N < hit[0] else hit[1] for hit in hits.items()}\n",
    "        n = len(samples)\n",
    "        return {hit[0]: hit[1]/n for hit in hits.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "xZDOdv3dN_XC",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1666036440980,
     "user_tz": -180,
     "elapsed": 368,
     "user": {
      "displayName": "Григорий Ксенофонтов",
      "userId": "05861251971421218806"
     }
    },
    "outputId": "2a4b4db3-48c6-4d57-c587-c3fb47b09ad5",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 15.22it/s]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{1: 0.2, 5: 0.3, 10: 0.5}"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "scorer = Scorer([1, 5, 10], embedder=embedder)\n",
    "scorer(validation[:10], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "uQ2Y9-SFHeXg",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1666036687009,
     "user_tz": -180,
     "elapsed": 246033,
     "user": {
      "displayName": "Григорий Ксенофонтов",
      "userId": "05861251971421218806"
     }
    },
    "outputId": "0604d5ba-c032-49aa-e4f0-b6febd66f785",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 3760/3760 [04:05<00:00, 15.30it/s]\n"
     ]
    }
   ],
   "source": [
    "scorer = Scorer(\n",
    "    k=[1, 5, 10, 100, 500, 1000],\n",
    "    embedder=embedder\n",
    ")\n",
    "\n",
    "hits = scorer(validation, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "WjXc5rBBgcj0",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1666036687010,
     "user_tz": -180,
     "elapsed": 19,
     "user": {
      "displayName": "Григорий Ксенофонтов",
      "userId": "05861251971421218806"
     }
    },
    "outputId": "f155dc53-2c2e-4abd-8056-82bf5105e340",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{1: 0.23297872340425532, 5: 0.3398936170212766, 10: 0.3941489361702128, 100: 0.598404255319149, 500: 0.8295212765957447, 1000: 1.0}\n"
     ]
    }
   ],
   "source": [
    "print(hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "axnkf7cwHeXg",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1666036687010,
     "user_tz": -180,
     "elapsed": 5,
     "user": {
      "displayName": "Григорий Ксенофонтов",
      "userId": "05861251971421218806"
     }
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#task_tests.test_scorer(hits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sh8slUejHeXg",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Предобработка текста\n",
    "\n",
    "Как вы могли заметить, мы имеем дело с сырыми данными. Это означает, что там присутствует много опечаток, спецсимволов и заглавных букв. В нашем случае это все может привести к ситуации, когда для данных токенов нет предобученных векторов. Поэтому необходима предобработка.\n",
    "\n",
    "Вам требуется:\n",
    "- Перевести символы в нижний регистр;\n",
    "- Заменить символы пунктуации и всевозможные плохие символы на пробелы;\n",
    "- Удалить стопслова.\n",
    "- Удалить слова с длиной меньше трех букв\n",
    "\n",
    "Реализуйте предобработку текста, используя предложенный шаблон."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "J1dvTxkzHeXg",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1666036687010,
     "user_tz": -180,
     "elapsed": 5,
     "user": {
      "displayName": "Григорий Ксенофонтов",
      "userId": "05861251971421218806"
     }
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "    \n",
    "    \n",
    "class TextPreprocessor:\n",
    "    \n",
    "    def __init__(self, characters, min_word_length=0, stopwords=None):\n",
    "        \"\"\"\n",
    "            characters: список плохих символов\n",
    "            min_word_length: минимальная допустимая длина для слов\n",
    "            stopwords: множество фоновых слов\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        self.characters = set(characters)\n",
    "        self._bad_reg_exp = re.compile('[' + ''.join(\"\\\\\" + c for c in characters) + ']')\n",
    "        self.min_word_length = min_word_length\n",
    "        self.stopwords = stopwords\n",
    "\n",
    "    def __call__(self, text):\n",
    "        \"\"\"\n",
    "            text: текст для обработки\n",
    "            \n",
    "            returns: обработанный текст\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        text = text.lower()\n",
    "        # print('[{}]'.format('\\\\'.join(self.characters)))\n",
    "        text = self._bad_reg_exp.sub(' ', text)\n",
    "\n",
    "        sentence = text.split()\n",
    "        sentence = list(filter(lambda word: word not in self.stopwords and len(word) >= self.min_word_length, sentence))\n",
    "        return ' '.join(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "2jZRWS52HeXh",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1666036687010,
     "user_tz": -180,
     "elapsed": 5,
     "user": {
      "displayName": "Григорий Ксенофонтов",
      "userId": "05861251971421218806"
     }
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "task_tests.test_text_preprocessor(TextPreprocessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G847PcV3HeXi",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Множество фоновых слов можно взять из **nltk** с помощью `nltk.corpus.stopwords.words`, выкидываемые плохие символы и пунктуацию следует подобрать самостоятельно.\n",
    "\n",
    "Обработайте текст и продемонстрируйте улучшение качества:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "OBarwG-THeXi",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1666036703036,
     "user_tz": -180,
     "elapsed": 16030,
     "user": {
      "displayName": "Григорий Ксенофонтов",
      "userId": "05861251971421218806"
     }
    },
    "outputId": "c22e423a-c5e3-4515-b702-ab133c562198",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "    \n",
    "\n",
    "###########################\n",
    "### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "###########################\n",
    "sym_to_remove = ('?', '!', ',', '.' , ';', ':', '[', ']', '(', ')', '{', '}', \n",
    "                \"''\", '>', '<', '%', '^', '$', '*', '#', '@', '~', '/', '\\\\',\n",
    "               '+', '-', '=')\n",
    "scorer = Scorer(\n",
    "    k=[1, 5, 10, 100, 500, 1000],\n",
    "    embedder=embedder\n",
    ")\n",
    "preprocessor = TextPreprocessor(sym_to_remove, min_word_length=3, stopwords=set(stopwords.words('english')))\n",
    "preprocessed = []\n",
    "for sample in validation:\n",
    "  preprocessed.append(list(map(preprocessor, sample)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ige09dBQDVpA",
    "outputId": "ba20be8f-e224-4577-d420-34244ecc1eb9",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1666036887405,
     "user_tz": -180,
     "elapsed": 184388,
     "user": {
      "displayName": "Григорий Ксенофонтов",
      "userId": "05861251971421218806"
     }
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 3760/3760 [03:04<00:00, 20.42it/s]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{1: 0.3281914893617021,\n",
       " 5: 0.4678191489361702,\n",
       " 10: 0.5183510638297872,\n",
       " 100: 0.6914893617021277,\n",
       " 500: 0.8534574468085107,\n",
       " 1000: 1.0}"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "hits = scorer(preprocessed, verbose=True)\n",
    "hits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VeU5zwpGHeXj",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Одним из критериев получения полных баллов является значение **hits@500** $\\geqslant 0.82$ до предобработки текста и $\\geqslant 0.85$ после предобработки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jFc45aEGHeXj",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Часть 2. Представления для неизвестных слов. (4 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XYqomrJCHeXj",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Для того, что получить представления для неизвестного слова, воспользуемся следующим подходом:\n",
    "    \n",
    "1. Будем восстанавливать эмбеддинг неизвестного слова как сумму эмбеддингов буквенных триграмм. Например, слово where должно представляться суммой триграмм #wh, whe, her, ere, re#\n",
    "\n",
    "2. В качестве обучающих данных будем использовать слова, для которых есть эмбеддинг в модели. Будем обучать эмбеддинги триграмм по выборке эмбеддингов с помощью функционала MSE:\n",
    "\n",
    "$$L = \\sum_{w \\in W_{known}}\\| f_{\\theta}(w) - v_w \\|^2 \\to \\min_{\\theta}$$\n",
    "\n",
    "где:\n",
    "\n",
    "* $W_{known}$ — множество известных модели слов\n",
    "* $f_{\\theta}(w)$ — сумма эмбеддингов триграмм слова $w$\n",
    "* $v_w$ — эмбеддинг слова $w$\n",
    "* $\\theta$ — веса эмбеддингов триграмм"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ePqaE6sHeXj",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Создание триграммного токенизатора\n",
    "\n",
    "Для начала, нам нужно:\n",
    "1. Пройтись по известным в word2vec словам и составить множество триграмм, для которых будем обучать векторы\n",
    "2. Составить маппинг из триграмм в индексы\n",
    "3. Реализовать преобразование произвольного слова в список триграмм\n",
    "4. Реализовать преобразование произвольного слова в список индексов триграмм\n",
    "\n",
    "Для реализации всех этих пунктов предлагается использовать шаблон, приведенный ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "K4PMBYUrHeXj",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1666036887405,
     "user_tz": -180,
     "elapsed": 7,
     "user": {
      "displayName": "Григорий Ксенофонтов",
      "userId": "05861251971421218806"
     }
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class TrigramTokenizer:\n",
    "    \n",
    "    def __init__(self, words):\n",
    "        \"\"\"\n",
    "            Формируем множество всевозможных триграмм, встречающихся в словах из words.\n",
    "            Делаем маппинг триграмм в индексы.\n",
    "            \n",
    "            words: список слов\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        self.trigramms = []\n",
    "        for word in tqdm.tqdm(words):\n",
    "          for trigramm in TrigramTokenizer._get_trigrams(word):\n",
    "            if trigramm not in self.trigramms:\n",
    "              self.trigramms.append(trigramm)\n",
    "\n",
    "        \n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        \"\"\"\n",
    "            returns: колчиество триграмм, для которых мы завели индекс.\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        return len(self.trigramms)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _get_trigrams(word):\n",
    "        \"\"\"\n",
    "            word: слово\n",
    "            \n",
    "            returns: список триграмм для word\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        if len(word) < 2:\n",
    "            return []\n",
    "\n",
    "        trigramms = []\n",
    "        for i in range(len(word)):\n",
    "          if i == 0:\n",
    "            trigram = \"\".join(['#', word[i], word[i + 1]])\n",
    "          elif i == len(word) - 1:\n",
    "            trigram = \"\".join([word[i - 1], word[i], '#'])\n",
    "          else:\n",
    "            trigram = \"\".join([word[i - 1], word[i], word[i + 1]])\n",
    "          trigramms.append(trigram)\n",
    "        return trigramms\n",
    "        \n",
    "    def __call__(self, word):\n",
    "        \"\"\"\n",
    "            word: слово\n",
    "            \n",
    "            returns: список индексов триграмм для слова word, которые нашлись в маппинге\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        filtered = filter(lambda x: x in self.trigramms, TrigramTokenizer._get_trigrams(word))\n",
    "        return list(map(lambda x: self.trigramms.index(x), filtered))\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "G1TpY_8EHeXk",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1666036887405,
     "user_tz": -180,
     "elapsed": 6,
     "user": {
      "displayName": "Григорий Ксенофонтов",
      "userId": "05861251971421218806"
     }
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "task_tests.test_trigram_tokenizer(TrigramTokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0r0N2s1kHeXk",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Для создания токенизатора используйте обработанный с помощью TextProcessor текст. \n",
    "\n",
    "**Важно:** в токенизатор нужно подавать только слова, известные word2vec'у."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "-RpXAiH8VO4g",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1666036887406,
     "user_tz": -180,
     "elapsed": 7,
     "user": {
      "displayName": "Григорий Ксенофонтов",
      "userId": "05861251971421218806"
     }
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# wv_embeddings_dict = dict(zip(map(preprocessor, wv_embeddings.index2entity), wv_embeddings.vectors))\n",
    "wv_embeddings_dict = dict(zip(wv_embeddings.vocab.keys(), wv_embeddings.vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y5pSldk5HeXk",
    "outputId": "486c2a6d-5b19-49fd-d624-67af5e03b692",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1666037211429,
     "user_tz": -180,
     "elapsed": 324030,
     "user": {
      "displayName": "Григорий Ксенофонтов",
      "userId": "05861251971421218806"
     }
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 500000/500000 [05:16<00:00, 1578.51it/s]\n"
     ]
    }
   ],
   "source": [
    "###########################\n",
    "### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "###########################\n",
    "# w2v_vocab = map(preprocessor, wv_embeddings.vocab.keys())\n",
    "w2v_vocab = list(wv_embeddings.vocab.keys())\n",
    "tri_tokenizer = TrigramTokenizer(w2v_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FLz97w2YHeXl",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Создание датасета с w2v векторами и списками индексов триграмм\n",
    "\n",
    "Мы будем обучать триграммную модель в нейросетевом фреймворке pytorch. Для этого нам нужно создать свой датасет.\n",
    "\n",
    "Он должен:\n",
    "1. Принимать список слов, word2vec и уже созданный триграммный токенизатор.\n",
    "2. Выдавать пары вида (эмбеддинг для слова из word2vec, список индексов триграмм для этого слова)\n",
    "\n",
    "Реализовать датасет нужно в шаблоне, приведенном ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "qZQf5SfXHeXl",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1666037211430,
     "user_tz": -180,
     "elapsed": 19,
     "user": {
      "displayName": "Григорий Ксенофонтов",
      "userId": "05861251971421218806"
     }
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class TrainTrigramDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, vocab, w2v_embeddings, tri_tokenizer):\n",
    "        \"\"\"\n",
    "            Формируем выборку для обучения триграммной модели.\n",
    "            ЗАРАНЕЕ считаем маппинг в список индексов для всех известных в word2vec слов.\n",
    "            \n",
    "            vocab: список слов\n",
    "            w2v_embeddings: no comments\n",
    "            tri_tokenizer: токенизатор триграмм\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        self.vocab = vocab\n",
    "        self.w2v_embeddings = w2v_embeddings\n",
    "        self.tri_tokenizer = tri_tokenizer \n",
    "                \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "            returns: возвращает количество слов, вошедших в маппинг (размер словаря)\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        return self.tri_tokenizer.vocab_size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "            returns: w2v эмбеддинг для idx-го слова в датасете, список соответствующих ему триграмм (тензоры)\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        return self.w2v_embeddings[self.vocab[idx]], self.tri_tokenizer(self.vocab[idx])\n",
    "\n",
    "    \n",
    "ds = TrainTrigramDataset(list(w2v_vocab), wv_embeddings_dict, tri_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "bXSwBSicHeXl",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1666037211430,
     "user_tz": -180,
     "elapsed": 18,
     "user": {
      "displayName": "Григорий Ксенофонтов",
      "userId": "05861251971421218806"
     }
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "task_tests.test_dataset(ds, list(w2v_vocab), wv_embeddings_dict, tri_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_f858jFMHeXl",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Создание DataLoader'а и Collator'а\n",
    "\n",
    "Нас интересуют в первую очередь четыре параметра при создании DataLoader:\n",
    "1. Датасет. Реализует интерфейс массива - можно узнать длину и получить элемент с индексом, меньшим длины.\n",
    "2. batch_size. Задает размера батча (количества сэмплов, идущих одновременно в модель).\n",
    "3. shuffle. При shuffle == True каждую эпоху при итерировании по даталоадеру мы будем получать сэмплы в произвольном порядке.\n",
    "4. collate_fn. Этот параметр позволяет задать кастомную логику \"склеивания\" сэмплов из датасета в батч.\n",
    "\n",
    "В качестве модели мы будем использовать слой **torch.nn.EmbeddingBag**. Он принимает на вход список индексов и список сдвигов, начинающийся с нуля.\n",
    "\n",
    "Нужно наш список списков индексов триграмм превратить в соответствующий формат, преобразовать векторы слов и два списка (индексов и сдвигов) в pytorch тензоры (torch.tensor).\n",
    "\n",
    "Реализуйте следующую функцию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "sRcIZAUdHeXl",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1666037211430,
     "user_tz": -180,
     "elapsed": 17,
     "user": {
      "displayName": "Григорий Ксенофонтов",
      "userId": "05861251971421218806"
     }
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "        batch: список из элементов датасета, e.g. [ds[i] for i in [2, 3, 1, 15]]\n",
    "        \n",
    "        returns: w2v эмбеддинги, индексы триграмм, сдвиги для триграмм\n",
    "    \"\"\"\n",
    "    ###########################\n",
    "    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "    ###########################\n",
    "    w2vs = []\n",
    "    tri_idxs = []\n",
    "    offests = [0]\n",
    "    for item in batch:\n",
    "      w2vs.append(item[0])\n",
    "      tri_idxs.extend(item[1])\n",
    "      offests.append(offests[-1] + len(item[1]))\n",
    "    offests.pop()\n",
    "    return torch.tensor(w2vs), torch.tensor(tri_idxs), torch.tensor(offests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9sbI3PyhHeXm",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1666037211430,
     "user_tz": -180,
     "elapsed": 17,
     "user": {
      "displayName": "Григорий Ксенофонтов",
      "userId": "05861251971421218806"
     }
    },
    "outputId": "a7430fb7-ffeb-44bd-c0bd-bf21b693f73c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:22: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n"
     ]
    }
   ],
   "source": [
    "task_tests.test_dataloader(ds, collate_fn, embedding_dim=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wKE_Ib25HeXm",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Создание модели\n",
    "\n",
    "При создании модели мы обычно наследуемся от **torch.nn.Module** и создаем нужные нам слои как атрибуты объекта нашего класса.\n",
    "\n",
    "В данном случае предлагается для формирования эмбеддингов использовать **torch.nn.EmbeddingBag**.\n",
    "\n",
    "Реализуйте предложенный шаблон:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "FXDiwAxwHeXm",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1666037211431,
     "user_tz": -180,
     "elapsed": 15,
     "user": {
      "displayName": "Григорий Ксенофонтов",
      "userId": "05861251971421218806"
     }
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class TrigramModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_embeddings, embedding_dim):\n",
    "        \"\"\"\n",
    "            num_embeddings: количество триграмм, для которых обучаются эмбеддинги\n",
    "            embedding_dim: размерность эмбеддингов триграмм\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        self._num_embeddings = num_embeddings\n",
    "        self._embedding_dim = embedding_dim\n",
    "        super().__init__()\n",
    "        self.net = nn.EmbeddingBag(num_embeddings, embedding_dim)\n",
    "        \n",
    "    @property\n",
    "    def embedding_dim(self):\n",
    "        \"\"\"\n",
    "            returns: размерность эмбеддингов\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        return self._embedding_dim\n",
    "    \n",
    "    @property\n",
    "    def num_embeddings(self):\n",
    "        \"\"\"\n",
    "            returns: количество эмбеддингов\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        return self._num_embeddings\n",
    "    \n",
    "    def forward(self, trigrams, offsets):\n",
    "        \"\"\"\n",
    "            trigrams: список индексов триграмм (тензор)\n",
    "            offsets: список сдвигов (тензор)\n",
    "            \n",
    "            returns: эмбеддинги слов, составленные из триграмм\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        out = self.net(trigrams, offsets)\n",
    "        return out.double()\n",
    "\n",
    "    \n",
    "model = TrigramModel(tri_tokenizer.vocab_size, embedding_dim=wv_embeddings.vector_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "JeKF8zQ9HeXm",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1666037211431,
     "user_tz": -180,
     "elapsed": 15,
     "user": {
      "displayName": "Григорий Ксенофонтов",
      "userId": "05861251971421218806"
     }
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "task_tests.test_trigram_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TUGaaQ1qHeXm",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Создание пайплайна обучения\n",
    "\n",
    "Далее необходимо совместить все наработки в единый пайплайн обучения, добавив также критерий для оптимизации и оптимизатор. \n",
    "\n",
    "Предлагается:\n",
    "\n",
    "1. В качестве оптимизатора использовать Adam (можно попробовать подобрать learning rate / weight decay)\n",
    "2. В качестве критерия оптимизации взять nn.MSELoss (можно также закодить лосс самому)\n",
    "3. Для даталоадера выбрать небольшой батч сайз (32, 64, 128, 256)\n",
    "4. Десяти эпох должно быть достаточно для хорошего качества\n",
    "\n",
    "Реализуйте предложенный шаблон."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RyIOdZ7PHeXn",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1666037213632,
     "user_tz": -180,
     "elapsed": 2216,
     "user": {
      "displayName": "Григорий Ксенофонтов",
      "userId": "05861251971421218806"
     }
    },
    "outputId": "50ca9e3f-7b69-4713-9eb6-2de3f91c713d",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from torch import optim\n",
    "\n",
    "    \n",
    "class Trainer:\n",
    "    \n",
    "    def __init__(self, model, criterion, optimizer):\n",
    "        \"\"\"\n",
    "            model: триграммная модель\n",
    "            criterion: функционал ошибки, принимает на вход w2v эмбеддинги и триграммные эмбеддинги\n",
    "            optimizer: оптимизатор для модели\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "        \n",
    "    def _train_step(self, dataloader):\n",
    "        \"\"\"\n",
    "            Делаем один проход по даталоадеру, с бэкпропом\n",
    "            \n",
    "            dataloader: даталоадер с тренировочными данными\n",
    "            \n",
    "            returns: лосс\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        for emb, idx, offset in tqdm.tqdm(dataloader):\n",
    "          emb = emb.to(self.device)\n",
    "          idx = idx.to(self.device)\n",
    "          offset = offset.to(self.device)\n",
    "          self.optimizer.zero_grad()\n",
    "          out = self.model(idx, offset)\n",
    "          loss = self.criterion(out, emb.double())\n",
    "          loss.backward()\n",
    "          self.optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "    def train(self, dataloader, n_epochs, verbose=False):\n",
    "        \"\"\"\n",
    "            dataloader: тренировочный даталоадер\n",
    "            n_epochs: количество эпох\n",
    "            verbose: выводить лосс каждую эпоху или нет\n",
    "            \n",
    "            returns: список лоссов\n",
    "        \"\"\"\n",
    "        start = time.time()\n",
    "        losses = []\n",
    "        for epoch in range(n_epochs):\n",
    "            loss = self._train_step(dataloader)\n",
    "            losses.append(loss)\n",
    "            if verbose:\n",
    "                print(f'epoch: {epoch + 1:>2}, loss: {loss:.4f}, time: {time.time() - start:.4f}')\n",
    "        return losses\n",
    "\n",
    "\n",
    "###########################\n",
    "### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "###########################\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model = TrigramModel(tri_tokenizer.vocab_size, embedding_dim=wv_embeddings.vector_size).to(device)\n",
    "trainer = Trainer(model, nn.MSELoss(), optim.Adam(model.parameters(), lr=0.05))\n",
    "dataloader = DataLoader(ds, 128, True, collate_fn=collate_fn, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XnR_dkxIqY6a",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1666037512144,
     "user_tz": -180,
     "elapsed": 298518,
     "user": {
      "displayName": "Григорий Ксенофонтов",
      "userId": "05861251971421218806"
     }
    },
    "outputId": "cd532d77-4488-41a9-c3a8-b99364f39ad5",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 428/428 [00:29<00:00, 14.34it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch:  1, loss: 0.0822, time: 29.8590\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 428/428 [00:31<00:00, 13.52it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch:  2, loss: 0.0518, time: 61.5331\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 428/428 [00:29<00:00, 14.33it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch:  3, loss: 0.0437, time: 91.4235\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 428/428 [00:30<00:00, 14.15it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch:  4, loss: 0.0378, time: 121.6767\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 428/428 [00:30<00:00, 14.21it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch:  5, loss: 0.0378, time: 151.8138\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 428/428 [00:29<00:00, 14.36it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch:  6, loss: 0.0439, time: 181.6323\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 428/428 [00:29<00:00, 14.32it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch:  7, loss: 0.0429, time: 211.5287\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 428/428 [00:31<00:00, 13.57it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch:  8, loss: 0.0529, time: 243.0879\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 428/428 [00:29<00:00, 14.32it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch:  9, loss: 0.0442, time: 272.9835\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 428/428 [00:29<00:00, 14.30it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch: 10, loss: 0.0451, time: 302.9195\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0.08217375979746766,\n",
       " 0.05184470402124019,\n",
       " 0.04366217737976522,\n",
       " 0.037773075982692444,\n",
       " 0.03779639360080277,\n",
       " 0.04392957690525796,\n",
       " 0.042851675579719535,\n",
       " 0.05285354578981311,\n",
       " 0.044162892548117394,\n",
       " 0.04514925571556161]"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    " trainer.train(dataloader, 10, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A0CihXt2HeXn",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Получение векторов неизвестных слов. Инференс модели\n",
    "\n",
    "Теперь, когда мы обучили модель, нам необходимо применить её для всех неизвестных слов, т.е. получить для них эмбеддинги.\n",
    "\n",
    "Т.к. для этих слов у нас нет word2vec эмбеддингов, то dataset и collator для обучения не подходят для инференса. Необходимо реализовать датасет и коллатор для инференса по следующим шаблонам:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "_56wCINoHeXn",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1666037512145,
     "user_tz": -180,
     "elapsed": 16,
     "user": {
      "displayName": "Григорий Ксенофонтов",
      "userId": "05861251971421218806"
     }
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class InferenceTrigramDataset:\n",
    "    \n",
    "    def __init__(self, vocab, tri_tokenizer):\n",
    "        \"\"\"\n",
    "            Датасет с неизвестными словами\n",
    "            \n",
    "            vocab: список слов\n",
    "            tri_tokenizer: триграммный токенизатор\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        self.vocab = vocab\n",
    "        self.tri_tokenizer = tri_tokenizer\n",
    "        \n",
    "    def __len__(self):\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        return len(self.vocab)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        return self.vocab[idx], self.tri_tokenizer(self.vocab[idx])\n",
    "    \n",
    "    \n",
    "def inference_collate_fn(trigrams):\n",
    "    \"\"\"\n",
    "        trigrams: список списков индексов триграмм\n",
    "        \n",
    "        returns: список индексов, список сдвигов триграмм \\\\\\ Я добавил слова для удобства\n",
    "    \"\"\"\n",
    "    ###########################\n",
    "    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "    ###########################\n",
    "    idxs = []\n",
    "    offests = [0]\n",
    "    words = []\n",
    "    for word, idx in trigrams:\n",
    "      words.append(word)\n",
    "      idxs.extend(idx)\n",
    "      offests.append(offests[-1] + len(idx)) \n",
    "    offests.pop()\n",
    "    return words, torch.tensor(idxs), torch.tensor(offests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NxxYMF4NHeXn",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Теперь у нас есть всё необходимое, чтобы осуществить инференс. Не забудь перед инференсом перевести модель в режим эвала (**model.eval**), а также использовать контекстный менеджер **torch.no_grad**.\n",
    "\n",
    "После инференса сформируйте словарь из известных в word2vec слов и их эмбеддингов, затем дополните его эмбеддингами для неизвестных слов, полученными после инференса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TxEwzEB4PTFd",
    "outputId": "3c7f3b2d-d957-400a-c2bc-52a3e4370e05",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1666037518132,
     "user_tz": -180,
     "elapsed": 6002,
     "user": {
      "displayName": "Григорий Ксенофонтов",
      "userId": "05861251971421218806"
     }
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 3760/3760 [00:06<00:00, 544.95it/s]\n"
     ]
    }
   ],
   "source": [
    "unknown = set()\n",
    "w2v_vocab_set = set(w2v_vocab)\n",
    "for sample in tqdm.tqdm(preprocessed):\n",
    "  # sample = map(preprocessor, sample)\n",
    "  for sentence in sample:\n",
    "    words = set(filter(lambda word: word not in w2v_vocab_set, sentence.split()))\n",
    "    unknown.update(words)\n",
    "unknown = list(unknown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "rK__cJZDHeXn",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1666037538478,
     "user_tz": -180,
     "elapsed": 20357,
     "user": {
      "displayName": "Григорий Ксенофонтов",
      "userId": "05861251971421218806"
     }
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "###########################\n",
    "### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "###########################\n",
    "model.cpu().eval()\n",
    "new_ebeddings = {}\n",
    "infDS = InferenceTrigramDataset(unknown, tri_tokenizer)\n",
    "dl = DataLoader(infDS, shuffle=True, collate_fn=inference_collate_fn)\n",
    "with torch.no_grad():\n",
    "  for word, idx, offset in dl:\n",
    "    if idx.nelement() != 0:\n",
    "      embedding = model(idx, offset)\n",
    "      new_ebeddings[word[0]] = embedding.numpy().squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cEVRKHjJCuJ9",
    "outputId": "5bb473c7-014b-489d-c3fb-455c5141e9d1",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1666037538478,
     "user_tz": -180,
     "elapsed": 24,
     "user": {
      "displayName": "Григорий Ксенофонтов",
      "userId": "05861251971421218806"
     }
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['typenamehandling', '\"bold\"', 'idbtransaction', 'backgroundresource', 'subviews', 'cassandra', 'sl4a', '`__gnu_cxx', 'patchvalue', '`user_registration_path']\n"
     ]
    }
   ],
   "source": [
    "print(unknown[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "1oPJ7F5datoi",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1666037538478,
     "user_tz": -180,
     "elapsed": 21,
     "user": {
      "displayName": "Григорий Ксенофонтов",
      "userId": "05861251971421218806"
     }
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "  w2v_trigram = {**new_ebeddings, **wv_embeddings_dict}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1JHuiAE0HeXo",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Используя **Scorer** и **Embedder**, получите новые значения метрик для валидации:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qyu9vFAbHeXo",
    "outputId": "8888ca76-2253-4ea7-b6ab-b5856a1d006c",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1666037665972,
     "user_tz": -180,
     "elapsed": 127515,
     "user": {
      "displayName": "Григорий Ксенофонтов",
      "userId": "05861251971421218806"
     }
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 3760/3760 [02:07<00:00, 29.44it/s]\n"
     ]
    }
   ],
   "source": [
    "###########################\n",
    "### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "###########################\n",
    "embedder = Embedder(w2v_trigram, dim=300)\n",
    "scorer = Scorer(\n",
    "    k=[1, 5, 10, 100, 500, 1000],\n",
    "    embedder=embedder\n",
    ")\n",
    "\n",
    "hits = scorer(preprocessed, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pZ1_zS5bdFIv",
    "outputId": "fe9e7132-1270-417c-c2fe-3c5382fb358d",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1666037666611,
     "user_tz": -180,
     "elapsed": 650,
     "user": {
      "displayName": "Григорий Ксенофонтов",
      "userId": "05861251971421218806"
     }
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{1: 0.4023936170212766,\n",
       " 5: 0.550531914893617,\n",
       " 10: 0.6,\n",
       " 100: 0.7587765957446808,\n",
       " 500: 0.9050531914893617,\n",
       " 1000: 1.0}"
      ]
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "hits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4AXB4OAXHeXo",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Одним из критериев получения полных баллов является значение метрики **hits@500** $\\geqslant 0.89$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O4UH4Fr_HeXo",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Часть 3. Обучение векторных представлений для целевой задачи. (4 баллов)\n",
    "\n",
    "Предполагается, что в этой части используются TextPreprocessor, Embedder, Scorer из предыдущих частей.\n",
    "\n",
    "Для обучения на целевую задачу нам понадобится обучающая выборка. Считайте её с диска, предобработайте текст вопросов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WLU85kdiHeXo",
    "outputId": "a1ee9c07-36e3-4163-827b-08abe87a4510",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1666037681894,
     "user_tz": -180,
     "elapsed": 15286,
     "user": {
      "displayName": "Григорий Ксенофонтов",
      "userId": "05861251971421218806"
     }
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 1000000/1000000 [00:11<00:00, 85852.03it/s]\n"
     ]
    }
   ],
   "source": [
    "train = []\n",
    "for questions in tqdm.tqdm(read_corpus('/content/drive/MyDrive/Colab Notebooks/MMTA FALL 2022/data/train.tsv')):\n",
    "    train.append([preprocessor(text) for text in questions])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z3wtiQqtHeXo",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Необходимо создать **токенизатор для текста** - составить словарь и сделать маппинг из слов в индексы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "8dr20ynVHeXp",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1666037681895,
     "user_tz": -180,
     "elapsed": 16,
     "user": {
      "displayName": "Григорий Ксенофонтов",
      "userId": "05861251971421218806"
     }
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class TextTokenizer:\n",
    "    \n",
    "    def __init__(self, vocab):\n",
    "        \"\"\"\n",
    "            vocab: множество слов, встретившихся в обучающей выборке\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        self.vocab = vocab\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        \"\"\"\n",
    "            returns: количество слов в словаре\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        return len(self.vocab)\n",
    "    \n",
    "    def __call__(self, text):\n",
    "        \"\"\"\n",
    "            text: текст\n",
    "            \n",
    "            returns: список индексов\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        filtered = filter(lambda x: x in self.vocab, text.split())\n",
    "        return list(map(self.vocab.index, filtered))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HiuL_gq2HeXp",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Составление словаря и токенизатора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PQvaPw0hHeXp",
    "outputId": "f709ca87-43a5-40a1-b41a-5b54ad848563",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1666037687261,
     "user_tz": -180,
     "elapsed": 5381,
     "user": {
      "displayName": "Григорий Ксенофонтов",
      "userId": "05861251971421218806"
     }
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 1000000/1000000 [00:04<00:00, 213212.06it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "There are 162379 words\n",
      "There are 81322 filtered words\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "###########################\n",
    "### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "###########################\n",
    "# vocab = set()\n",
    "# for sample in tqdm.tqdm(train):\n",
    "#   for text in sample:\n",
    "#     vocab.update(text.split(' '))\n",
    "\n",
    "# vocab = list(vocab)\n",
    "vocab = dict()\n",
    "for sample in tqdm.tqdm(train):\n",
    "  for text in sample:\n",
    "    for word in text.split(' '):\n",
    "      if word not in vocab:\n",
    "        vocab[word] = 1\n",
    "      else:\n",
    "        vocab[word] +=  1\n",
    "print(f'There are {len(vocab)} words')\n",
    "sorted_values = list(sorted(vocab.values()))\n",
    "left_thr = sorted_values[int(0.5 * len(vocab))]\n",
    "right_thr = sorted_values[min(int(1 * len(vocab)), len(vocab) - 1)]\n",
    "fil_words = list(filter(lambda word: vocab[word] <= right_thr and vocab[word] >= left_thr, vocab))\n",
    "print(f'There are {len(fil_words)} filtered words')\n",
    "                \n",
    "vocab = fil_words\n",
    "\n",
    "textTokenizer = TextTokenizer(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fqd18sVOHeXq",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Нам также понадобится **новый датасет для обучения**. Для применения метода NT-Exent нам не нужно \"майнить негативы\", поэтому датасет надо сформировать как массив из пар-дубликатов.\n",
    "\n",
    "Так как данные в обучающей выборке содержат множества дубликатов (т.е. все дубликаты сгруппированы в списки), есть несколько способов сформировать итоговый датасет:\n",
    "1. Оставить из каждого множества дубликатов какие-нибудь случайные два (или просто первые два вопроса)\n",
    "2. Для первого вопроса в множестве взять все остальные как дубликаты (N вопросов-дубликатов - N-1 пара). Тогда мы увидим каждый вопрос хотя бы один раз при обучении\n",
    "3. Составить всевозможные уникальные пары-дубликаты из этих множеств (т.е. первый вопрос и все остальные вопросы, второй вопрос и все остальные, кроме первого).\n",
    "\n",
    "Каждый следующий способ, начиная с первого, раздувает выборку по размеру, но возможно дает прирост к качеству решения задачи.\n",
    "\n",
    "Реализуйте выбранный вами подход, используя предолженный шаблон:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "vZc2GnaxHeXr",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1666037687262,
     "user_tz": -180,
     "elapsed": 13,
     "user": {
      "displayName": "Григорий Ксенофонтов",
      "userId": "05861251971421218806"
     }
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class QuestionDuplicatesDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, question_pairs, tokenizer):\n",
    "        \"\"\"\n",
    "            question_pairs: список из пар вопросов-дубликатов\n",
    "            tokenizer: объект класса TextTokenizer\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        self.question_pairs = question_pairs\n",
    "        self.tokenizer = tokenizer\n",
    "            \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "            returns: количество пар-дубликатов\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        return len(self.question_pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "            returns: (вопрос, дубликат), idx-ю пару в датасете\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        return list(map(self.tokenizer, self.question_pairs[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q8w9TuUKHeXr",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Также нужно подготовить **даталоадер** (а именно - коллатор для даталоадера) по аналогии со второй частью задания."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "mDvX5MlUHeXr",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1666037687262,
     "user_tz": -180,
     "elapsed": 12,
     "user": {
      "displayName": "Григорий Ксенофонтов",
      "userId": "05861251971421218806"
     }
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_ids_and_offsets(questions):\n",
    "    \"\"\"\n",
    "        questions: список из токенизированных вопросов\n",
    "        \n",
    "        returns: (ids, offsets), где ids - вытянутый список индексов слов в вопросах из батча, offsets - сдвиги\n",
    "    \"\"\"\n",
    "    ###########################\n",
    "    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "    ###########################\n",
    "    ids = []\n",
    "    offsets = [0]\n",
    "    for question in questions:\n",
    "      ids.extend(question)\n",
    "      offsets.append(offsets[-1] + len(question))\n",
    "    offsets.pop()\n",
    "    return torch.tensor(ids), torch.tensor(offsets)\n",
    "\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "        batch: список из пар токенизированных вопросов-дубликатов [(question, duplicate), ...]\n",
    "        \n",
    "        returns: (question_ids, question_offsets), (duplicate_ids, duplicate_offsets)\n",
    "    \"\"\"\n",
    "    ###########################\n",
    "    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "    ###########################\n",
    "    questions, duplicates = zip(*batch)\n",
    "    return get_ids_and_offsets(questions), get_ids_and_offsets(duplicates)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "55_wmkFyHeXr",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Поделите выборку на трейн и валидацию, используя train_test_split, затем **создайте датасеты и даталоадеры** для обучения и валидации. Сколько пар-дубликатов получилось в датасете для обучения?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "zvFSiYRUHeXr",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1666038761446,
     "user_tz": -180,
     "elapsed": 1074196,
     "user": {
      "displayName": "Григорий Ксенофонтов",
      "userId": "05861251971421218806"
     }
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "###########################\n",
    "### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "###########################\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "ds = QuestionDuplicatesDataset(train, textTokenizer)\n",
    "ds_train, ds_test, _, _ = train_test_split(ds, [1] * len(ds), shuffle=False, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "k-sCkM2JvuYy",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1666038761447,
     "user_tz": -180,
     "elapsed": 23,
     "user": {
      "displayName": "Григорий Ксенофонтов",
      "userId": "05861251971421218806"
     }
    },
    "outputId": "a2132a21-e651-4a33-dd13-adbf6f8fdae4",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train dataset contains:\t 750000 samples\n",
      "Test dataset contains:\t 250000 samples\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    }
   ],
   "source": [
    "dl_train = DataLoader(ds_train, batch_size=2048, shuffle=True, collate_fn=collate_fn, num_workers=4)\n",
    "dl_test = DataLoader(ds_test, batch_size=2048, shuffle=False, collate_fn=collate_fn, num_workers=4)\n",
    "\n",
    "print(f'Train dataset contains:\\t {len(ds_train)} samples')\n",
    "print(f'Test dataset contains:\\t {len(ds_test)} samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c5xikZCdHeXr",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "С помощью предложенного шаблона **задайте модель** для преобразования вопросов в векторы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "CL7wZnjVHeXs",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1666040894425,
     "user_tz": -180,
     "elapsed": 395,
     "user": {
      "displayName": "Григорий Ксенофонтов",
      "userId": "05861251971421218806"
     }
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class DssmLikeModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_embeddings, embedding_dim):\n",
    "        \"\"\"\n",
    "            num_embeddings: количество слов, для которых обучаем эмбеддинги\n",
    "            embedding_dim: размерность эмбеддинга\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.net = nn.EmbeddingBag(num_embeddings, embedding_dim)\n",
    "        \n",
    "    def forward(self, ids, offsets):\n",
    "        \"\"\"\n",
    "            ids: вытянутая посл-ть индексов слов вопросов, попавших в батч\n",
    "            offsets: сдвиги для вопросов, попавших в батч\n",
    "            \n",
    "            returns: векторы вопросов\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        return self.net(ids, offsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5uI48DyrHeXs",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Создание модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "7mNWigNWHeXs",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1666043072214,
     "user_tz": -180,
     "elapsed": 357,
     "user": {
      "displayName": "Григорий Ксенофонтов",
      "userId": "05861251971421218806"
     }
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "###########################\n",
    "### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "###########################\n",
    "model = DssmLikeModel(len(vocab), embedding_dim=wv_embeddings.vector_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CEAmmScXHeXs",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Критерий оптимизации** для NTExentLoss выглядит как:\n",
    "\n",
    "$$\\mathcal{L}(Q, D) = -0.5 \\log diag(softmax(QD^T / \\alpha)) - 0.5 \\log diag(softmax(DQ^T / \\alpha)),$$\n",
    "\n",
    "где:\n",
    "* $Q \\in \\mathbb{R}^{b \\times d}$ - эмбеддинги вопросов, \n",
    "* $D \\in \\mathbb{R}^{b \\times d}$ - эмбеддинги соответствующих вопросам дубликатов,\n",
    "* $b$ - количество пар (вопрос, дубликат), $d$ - размерность эмбеддингов, $\\alpha$ - гиперпараметр лосса. \n",
    "* Softmax берется по рядам\n",
    "* Матрицы $Q, D$ содержат нормированные эмбеддинги, т.е. считается именно косинус."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "0kvByZHQHeXs",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1666043072600,
     "user_tz": -180,
     "elapsed": 1,
     "user": {
      "displayName": "Григорий Ксенофонтов",
      "userId": "05861251971421218806"
     }
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class NTExentLoss(nn.Module):\n",
    "    \n",
    "    def __init__(self, alpha=1., eps=1e-8):\n",
    "        \"\"\"\n",
    "            alpha: коэффициент, на который мы делим скоры перед софтмаксом\n",
    "            eps: ||v|| = min(eps, ||v||)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        self.alpha = alpha\n",
    "        self.eps = eps\n",
    "        \n",
    "    def _normalize(self, embeddings):\n",
    "        \"\"\"\n",
    "            embeddings: матрица размера [batch_size, embedding_dim]\n",
    "            \n",
    "            returns: матрица такого же размера, но с нормироваными векторами\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        return embeddings / (torch.norm(embeddings, dim=1).reshape(-1, 1) + self.eps)\n",
    "    \n",
    "    def forward(self, embeddings, positives):\n",
    "        \"\"\"\n",
    "            embeddings: матрица размера [batch_size, embedding_dim]\n",
    "            positives: матрица такого же размера, с позитивами для векторов из матрицы embeddings\n",
    "            \n",
    "            returns: NT-Exent loss\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        embeddings = self._normalize(embeddings)\n",
    "        positives = self._normalize(positives)\n",
    "        return torch.mean(-0.5 * torch.log(torch.diag(torch.softmax(embeddings @ positives.t() / self.alpha, dim=1), 0)) - \\\n",
    "                          -0.5 * torch.log(torch.diag(torch.softmax(positives @ embeddings.t() / self.alpha, dim=1), 0)))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5sB1joxDHeXt",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Создайте пайплайн** для обучения и валидации, используя предложенный шаблон.\n",
    "\n",
    "Залогируйте с помощью **torch.utils.tensorboard.SummaryWriter** две величины:\n",
    "1. Лосс для каждого батча\n",
    "2. Лосс на валидации для каждой эпохи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "HYoLhzosHeXt",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1666043072916,
     "user_tz": -180,
     "elapsed": 2,
     "user": {
      "displayName": "Григорий Ксенофонтов",
      "userId": "05861251971421218806"
     }
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import shutil\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "    \n",
    "    \n",
    "class Trainer:\n",
    "    \n",
    "    def __init__(\n",
    "            self, \n",
    "            model, \n",
    "            optimizer, \n",
    "            criterion, \n",
    "            logdir=None, \n",
    "            device=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "            model: объект класса DssmModel\n",
    "            optimizer: оптимизатор\n",
    "            criterion: критерий оптимизации\n",
    "            logdir: директория, в которую SummaryWriter должен писать логи\n",
    "            device: девайс (cpu или cuda), на котором надо производить вычисления\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        self.device = device\n",
    "        if self.device is None:\n",
    "            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model = model.to(self.device)\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.criterion = self.criterion.to(self.device)\n",
    "        self.logdir = logdir\n",
    "        self._writer = SummaryWriter()\n",
    "    \n",
    "    def _calculate_loss(self, batch):\n",
    "        \"\"\"\n",
    "            batch: батч из индексов и сдвигов для вопросов и их дубликатов\n",
    "            \n",
    "            returns: посчитанный для батча лосс\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        (question_idx, question_offs), (duplicate_idx, duplicate_offs) = batch\n",
    "        return self.criterion(self.model(question_idx, question_offs),\n",
    "                              self.model(duplicate_idx, duplicate_offs))\n",
    "        \n",
    "    def _train_step(self, dataloader):\n",
    "        \"\"\"\n",
    "            dataloader: даталоадер для обучения\n",
    "            \n",
    "            returns: лосс на датасете для обучения\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        for (question_idx, question_offs), (duplicate_idx, duplicate_offs) in tqdm.tqdm(dataloader):\n",
    "            question_idx = question_idx.to(self.device)\n",
    "            question_offs = question_offs.to(self.device)\n",
    "            duplicate_idx = duplicate_idx.to(self.device)\n",
    "            duplicate_offs = duplicate_offs.to(self.device)\n",
    "            loss = self._calculate_loss(((question_idx, question_offs), (duplicate_idx, duplicate_offs)))\n",
    "            self._writer.add_scalar(\"Loss/train\", loss.detach().item())\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        return loss.detach().item()\n",
    "\n",
    "    def _eval_step(self, dataloader):\n",
    "        \"\"\"\n",
    "            dataloader: даталоадер для валидации\n",
    "            \n",
    "            returns: лосс на валидации\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        avg_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for i, ((question_idx, question_offs), (duplicate_idx, duplicate_offs)) in enumerate(dataloader):\n",
    "                question_idx = question_idx.to(self.device)\n",
    "                question_offs = question_offs.to(self.device)\n",
    "                duplicate_idx = duplicate_idx.to(self.device)\n",
    "                duplicate_offs = duplicate_offs.to(self.device)\n",
    "                avg_loss = (avg_loss * i + self._calculate_loss(((question_idx, question_offs), (duplicate_idx, duplicate_offs))).item()) / (i + 1)\n",
    "        return avg_loss\n",
    "    \n",
    "    def train(self, dataloaders, n_epochs, verbose=False):\n",
    "        \"\"\"\n",
    "            dataloaders: словарь вида {'train': train_dataloader, 'eval': eval_dataloader}\n",
    "            n_epochs: количество эпох обучения\n",
    "            verbose: нужно ли выводить каждую эпоху информацию про лоссы\n",
    "        \"\"\"\n",
    "        start = time.time()\n",
    "        for epoch in range(n_epochs):\n",
    "            train_loss = self._train_step(dataloaders['train'])\n",
    "            \n",
    "            eval_loss = self._eval_step(dataloaders['eval'])\n",
    "            if self._writer is not None:\n",
    "                self._writer.add_scalar('eval/loss', eval_loss)\n",
    "                \n",
    "            if verbose:\n",
    "                print(\n",
    "                    'epoch: {:>2}, train loss: {:.4f}, eval loss: {:.4f}, time: {:.4f}' \\\n",
    "                        .format(epoch + 1, train_loss, eval_loss, time.time() - start)\n",
    "                )\n",
    "                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "qU_MtGZKxoYY",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1666043072916,
     "user_tz": -180,
     "elapsed": 2,
     "user": {
      "displayName": "Григорий Ксенофонтов",
      "userId": "05861251971421218806"
     }
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-2, weight_decay=0.0)\n",
    "trainer = Trainer(model, optimizer, NTExentLoss(), 'training_log', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2VNxnqb5HeXt",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Предлагается использовать для оптимизации Адам и обучать модель 10-60 эпох.\n",
    "\n",
    "Для этой части задания GPU даёт существенное ускорение при обучении, поэтому стоит по возможности делать обучение с большим batch size'ом и на GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "OlGBvVtHHeXt",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1666043648304,
     "user_tz": -180,
     "elapsed": 574051,
     "user": {
      "displayName": "Григорий Ксенофонтов",
      "userId": "05861251971421218806"
     }
    },
    "outputId": "9c248fe7-968e-46dc-d774-7b78335568ab",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 367/367 [00:08<00:00, 44.94it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch:  1, train loss: 0.0020, eval loss: -0.0042, time: 9.5338\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 367/367 [00:08<00:00, 44.73it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch:  2, train loss: -0.0042, eval loss: -0.0059, time: 19.1362\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 367/367 [00:08<00:00, 44.95it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch:  3, train loss: -0.0095, eval loss: -0.0067, time: 28.7684\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 367/367 [00:09<00:00, 39.29it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch:  4, train loss: -0.0288, eval loss: -0.0076, time: 39.5198\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 367/367 [00:08<00:00, 44.94it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch:  5, train loss: -0.0164, eval loss: -0.0080, time: 49.0857\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 367/367 [00:08<00:00, 45.35it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch:  6, train loss: -0.0217, eval loss: -0.0083, time: 58.5573\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 367/367 [00:08<00:00, 45.16it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch:  7, train loss: -0.0330, eval loss: -0.0083, time: 68.1081\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 367/367 [00:08<00:00, 45.52it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch:  8, train loss: -0.0266, eval loss: -0.0085, time: 77.5881\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 367/367 [00:08<00:00, 45.22it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch:  9, train loss: -0.0339, eval loss: -0.0086, time: 87.0912\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 367/367 [00:08<00:00, 45.21it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch: 10, train loss: -0.0366, eval loss: -0.0089, time: 96.5726\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 367/367 [00:08<00:00, 45.31it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch: 11, train loss: -0.0271, eval loss: -0.0088, time: 106.0497\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 367/367 [00:08<00:00, 45.40it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch: 12, train loss: -0.0220, eval loss: -0.0088, time: 115.5041\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 367/367 [00:08<00:00, 45.33it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch: 13, train loss: -0.0258, eval loss: -0.0087, time: 125.0054\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 367/367 [00:08<00:00, 45.35it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch: 14, train loss: -0.0352, eval loss: -0.0090, time: 134.4912\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 367/367 [00:08<00:00, 45.57it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch: 15, train loss: -0.0321, eval loss: -0.0090, time: 143.9446\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 367/367 [00:08<00:00, 45.45it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch: 16, train loss: -0.0435, eval loss: -0.0089, time: 153.3889\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 367/367 [00:08<00:00, 45.48it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch: 17, train loss: -0.0384, eval loss: -0.0090, time: 162.8187\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 367/367 [00:08<00:00, 45.59it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch: 18, train loss: -0.0283, eval loss: -0.0090, time: 172.2237\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 367/367 [00:08<00:00, 40.80it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch: 19, train loss: -0.0353, eval loss: -0.0091, time: 182.6019\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 367/367 [00:08<00:00, 45.48it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch: 20, train loss: -0.0476, eval loss: -0.0092, time: 192.0727\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 367/367 [00:08<00:00, 45.80it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch: 21, train loss: -0.0498, eval loss: -0.0091, time: 201.4771\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 367/367 [00:08<00:00, 45.61it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch: 22, train loss: -0.0344, eval loss: -0.0092, time: 210.9076\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 367/367 [00:08<00:00, 45.53it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch: 23, train loss: -0.0273, eval loss: -0.0092, time: 220.3455\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 367/367 [00:08<00:00, 45.53it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch: 24, train loss: -0.0246, eval loss: -0.0093, time: 229.8036\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 367/367 [00:08<00:00, 45.19it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch: 25, train loss: -0.0334, eval loss: -0.0092, time: 239.3142\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 367/367 [00:08<00:00, 45.63it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch: 26, train loss: -0.0402, eval loss: -0.0093, time: 248.7581\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 367/367 [00:08<00:00, 45.49it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch: 27, train loss: -0.0560, eval loss: -0.0091, time: 258.2212\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 367/367 [00:08<00:00, 45.73it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch: 28, train loss: -0.0352, eval loss: -0.0091, time: 267.6167\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 367/367 [00:08<00:00, 45.38it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch: 29, train loss: -0.0269, eval loss: -0.0093, time: 277.0816\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 367/367 [00:08<00:00, 45.16it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch: 30, train loss: -0.0368, eval loss: -0.0093, time: 286.6578\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 367/367 [00:08<00:00, 45.05it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch: 31, train loss: -0.0386, eval loss: -0.0093, time: 296.2440\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 367/367 [00:08<00:00, 45.17it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch: 32, train loss: -0.0337, eval loss: -0.0093, time: 305.7947\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 367/367 [00:08<00:00, 44.82it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch: 33, train loss: -0.0507, eval loss: -0.0091, time: 315.3850\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 367/367 [00:08<00:00, 41.11it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch: 34, train loss: -0.0473, eval loss: -0.0093, time: 325.6966\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 367/367 [00:08<00:00, 45.31it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch: 35, train loss: -0.0490, eval loss: -0.0094, time: 335.2209\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 367/367 [00:08<00:00, 44.94it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch: 36, train loss: -0.0396, eval loss: -0.0095, time: 344.8066\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 367/367 [00:08<00:00, 45.19it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch: 37, train loss: -0.0515, eval loss: -0.0094, time: 354.3055\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 367/367 [00:08<00:00, 45.28it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch: 38, train loss: -0.0389, eval loss: -0.0095, time: 363.8353\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 367/367 [00:08<00:00, 44.45it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch: 39, train loss: -0.0387, eval loss: -0.0095, time: 373.5545\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 367/367 [00:08<00:00, 44.57it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch: 40, train loss: -0.0555, eval loss: -0.0096, time: 383.2080\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 367/367 [00:08<00:00, 44.80it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch: 41, train loss: -0.0521, eval loss: -0.0094, time: 392.8554\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 367/367 [00:08<00:00, 44.99it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch: 42, train loss: -0.0347, eval loss: -0.0094, time: 402.4103\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 367/367 [00:08<00:00, 45.01it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch: 43, train loss: -0.0522, eval loss: -0.0094, time: 411.9374\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 367/367 [00:08<00:00, 45.26it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch: 44, train loss: -0.0391, eval loss: -0.0092, time: 421.4615\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 367/367 [00:08<00:00, 45.07it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch: 45, train loss: -0.0410, eval loss: -0.0092, time: 431.0234\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 367/367 [00:08<00:00, 45.50it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch: 46, train loss: -0.0401, eval loss: -0.0094, time: 440.4721\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 367/367 [00:08<00:00, 45.48it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch: 47, train loss: -0.0307, eval loss: -0.0096, time: 449.9318\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 367/367 [00:08<00:00, 45.44it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch: 48, train loss: -0.0349, eval loss: -0.0094, time: 459.3953\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 367/367 [00:08<00:00, 41.10it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch: 49, train loss: -0.0326, eval loss: -0.0093, time: 469.7097\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 367/367 [00:08<00:00, 44.99it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch: 50, train loss: -0.0399, eval loss: -0.0095, time: 479.2511\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 367/367 [00:08<00:00, 45.26it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch: 51, train loss: -0.0388, eval loss: -0.0093, time: 488.7491\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 367/367 [00:08<00:00, 45.66it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch: 52, train loss: -0.0460, eval loss: -0.0093, time: 498.1934\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 367/367 [00:08<00:00, 45.42it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch: 53, train loss: -0.0482, eval loss: -0.0092, time: 507.6542\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 367/367 [00:08<00:00, 45.43it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch: 54, train loss: -0.0365, eval loss: -0.0093, time: 517.1159\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 367/367 [00:08<00:00, 45.48it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch: 55, train loss: -0.0427, eval loss: -0.0094, time: 526.5663\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 367/367 [00:08<00:00, 45.48it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch: 56, train loss: -0.0478, eval loss: -0.0093, time: 536.0299\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 367/367 [00:08<00:00, 45.26it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch: 57, train loss: -0.0399, eval loss: -0.0095, time: 545.5646\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 367/367 [00:08<00:00, 45.39it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch: 58, train loss: -0.0348, eval loss: -0.0094, time: 555.0637\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 367/367 [00:08<00:00, 45.52it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch: 59, train loss: -0.0529, eval loss: -0.0096, time: 564.4947\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 367/367 [00:08<00:00, 45.45it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch: 60, train loss: -0.0317, eval loss: -0.0096, time: 573.9479\n"
     ]
    }
   ],
   "source": [
    " ###########################\n",
    "### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "###########################\n",
    "trainer.train({'train': dl_train, 'eval': dl_test}, 60, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x9TLv48FHeXt",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Осталось достать из модели обученные под задачу векторы слов, составить маппинг слов в векторы, создать **Embedder** и **Scorer** и провалидировать качество на нашей исходной валидации, которой мы пользовались в первых двух частях.\n",
    "\n",
    "Чтобы достать из модели веса, можно использовать `model._embeddings.weight.cpu().detach().numpy()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "HZL9QqgDHeXu",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1666043783371,
     "user_tz": -180,
     "elapsed": 135080,
     "user": {
      "displayName": "Григорий Ксенофонтов",
      "userId": "05861251971421218806"
     }
    },
    "outputId": "01314f09-50bc-4d99-c40f-6053e728218a",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 3760/3760 [02:15<00:00, 27.84it/s]\n"
     ]
    }
   ],
   "source": [
    "###########################\n",
    "### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "###########################\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "embeddings = model.net.weight.cpu().detach().numpy()\n",
    "embedder = Embedder({word : emb for word, emb in zip(list(vocab), embeddings)}, dim=wv_embeddings.vector_size)\n",
    "scorer = Scorer(\n",
    "    k=[1, 5, 10, 100, 500, 1000],\n",
    "    embedder=embedder\n",
    ")\n",
    "\n",
    "hits = scorer(preprocessed, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "rbLl-RrS2ITz",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1666043783372,
     "user_tz": -180,
     "elapsed": 18,
     "user": {
      "displayName": "Григорий Ксенофонтов",
      "userId": "05861251971421218806"
     }
    },
    "outputId": "76b162c8-4a2b-4ff4-f8b7-8a972e30c1a5",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{1: 0.15824468085106383,\n",
       " 5: 0.2125,\n",
       " 10: 0.23031914893617023,\n",
       " 100: 0.34867021276595744,\n",
       " 500: 0.6590425531914894,\n",
       " 1000: 1.0}"
      ]
     },
     "metadata": {},
     "execution_count": 74
    }
   ],
   "source": [
    "hits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B-71TkkaHeXu",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Одним из критериев получения полных баллов является значение метрики **hits@500** $\\geqslant 0.98$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HQATOZ6WHeXu",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Дополнительная часть (до 3 баллов)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kGV_2E3ZHeXu",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Каждый из пунктов при успешном выполнении гарантирует как минимум один дополнительный балл. Максимум вам будет зачтено три пункта. \n",
    "1. Обучить триграммную модель на косинусную близость вместо евклидового расстояния, получить прирост качества (относительно триграммной модели с MSE)\n",
    "2. Обучить в качестве триграммной модели char-biLSTM вместо мешка векторов триграмм, получить прирост качества (относительно триграммной модели с таким же критерием оптимизации)\n",
    "3. Усложнить модель в части 3, добавить к мешку вектора словесных униграмм также мешок векторов словесных биграмм и мешок векторов буквенных триграмм (сделать модель более похожей на настоящий dssm), получить прирост качества\n",
    "4. Модифицировать модель в части 3 произвольным образом (добавить MLP, нормализации, дропаут, сделать bilstm поверх последовательности векторов слов, трансформер и т.д.), получить прирост качества\n",
    "5. Сделать модель с ранним связыванием (early fusion) - векторы вопросов конкатенируются и проходят через MLP (с возможными модификациями) перед созданием предсказания. Hint: возможно стоит предобучить эмбеддинги слов с помощью NT-Exent перед обучением финальной модели."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": [
    {
     "file_id": "1U-f_Ghy1cXUZG2LAnAiLZd6ZUG8mZrWV",
     "timestamp": 1665325217419
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}