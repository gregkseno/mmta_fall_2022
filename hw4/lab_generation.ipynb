{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOsFi7Rn9b8O"
      },
      "source": [
        "# Практическое задание 4\n",
        "# Генерация bash команды по текстовому запросу\n",
        "## курс \"Математические методы анализа текстов\"\n",
        "### ФИО: Ксенофонтов Григорий М05-204а\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5JmrhWD9b8Q"
      },
      "source": [
        "### Постановка задачи\n",
        "\n",
        "В этом задании вы построите систему, выдающую пользователю последовательность утилит командной строки linux (с нужными флагами) по его текстовому запросу. Вам дан набор пар текстовый запрос - команда на выходе. \n",
        "\n",
        "Решение этого задания будет построено на encoder-decoder архитектуре и модели transformer.\n",
        "\n",
        "\n",
        "### Библиотеки\n",
        "\n",
        "Для этого задания вам понадобятся следующие библиотеки:\n",
        "* pytorch\n",
        "* transformers\n",
        "* sentencepiece (bpe токенизация)\n",
        "* clai utils (скачать с гитхаба отсюда https://github.com/IBM/clai/tree/nlc2cmd/utils) \n",
        "\n",
        "\n",
        "### Данные\n",
        "\n",
        "В качестве обучающей выборке используются данные, сгенерированные автоматически по запросам с сайта stack overflow. В качестве тестовых данных используются пары запросов, размеченные асессорами.\n",
        "\n",
        "Данные можно скачать по ссылке: https://drive.google.com/file/d/1n457AAgrMwd5VbT6mGZ_rws3g2wwdEfX/view?usp=sharing\n",
        "\n",
        "### Метрика качества\n",
        "\n",
        "Ваш алгоритм должен выдавать пять вариантов ответа для каждого запроса. \n",
        "Для упрощения задачи метрика качества будет учитывать утилиты и флаги ответа, но не учитывать подставленные значения. Пусть $\\{ u_1, \\ldots, u_T \\}$, $\\{ f_1, \\ldots, f_T \\}$ --- список утилит и множества их флагов ответа алгоритма, $\\{v_1, \\ldots, v_T \\}$, $\\{ \\phi_1, \\ldots, \\phi_T \\}$ --- список утилит и множества их флагов эталонного ответа. Если ответы отличаются по длине, они дополняются `None` утилитой. \n",
        "\n",
        "$$ S = \\frac{1}{T} \\sum_{i=1}^{T} \\left(\\mathbb{I}[u_i = v_i]\\left( 1 + \\frac{1}{2}s(f_i, \\phi_i)\\right) - 1\\right)$$\n",
        "\n",
        "$$ s(f, \\phi) = 1 + \\frac{2 |f \\cap \\phi| - |f \\cup \\phi|}{\\max(|f|, |\\phi|)} $$\n",
        "\n",
        "Метрика учитывает, что предсказать правильную утилиту важнее чем правильный флаг. При этом порядок флагов не важен (однако, чтобы корректно "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juOAlpd99b8Q"
      },
      "source": [
        "## Предобработка данных (2 балла)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!mkdir /content/data | cp /content/drive/MyDrive/text2bash_data/test_data.csv /content/data/test.csv | cp /content/drive/MyDrive/text2bash_data/train.csv /content/data/train.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4Yk9QD4jdO9",
        "outputId": "bad994c8-b5bd-4e61-e870-101e386e8ca1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone -b nlc2cmd https://github.com/IBM/clai/\n",
        "!pip install sentencepiece transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3RmMSSJN-I5W",
        "outputId": "ec27f2d3-e22c-4c79-c734-29ee9757f020"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'clai'...\n",
            "remote: Enumerating objects: 4162, done.\u001b[K\n",
            "remote: Counting objects: 100% (262/262), done.\u001b[K\n",
            "remote: Compressing objects: 100% (206/206), done.\u001b[K\n",
            "remote: Total 4162 (delta 69), reused 207 (delta 46), pack-reused 3900\u001b[K\n",
            "Receiving objects: 100% (4162/4162), 131.85 MiB | 40.12 MiB/s, done.\n",
            "Resolving deltas: 100% (2538/2538), done.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers\n",
            "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m95.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m104.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.4/182.4 KB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Installing collected packages: tokenizers, sentencepiece, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.11.1 sentencepiece-0.1.97 tokenizers-0.13.2 transformers-4.25.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "yRZ_ZpI69b8R"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "PATH_TO_CLAI_UTILS = \"/content/clai/utils\" ## YOUR CODE HERE ##\n",
        "sys.path.append(PATH_TO_CLAI_UTILS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "F7471DJE9b8R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a8be0b9-c835-4370-d5d4-20cf38819fe3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting bashlex grammar using file: /content/clai/utils/bashlint/grammar/grammar100.txt\n",
            "Bashlint grammar set up (148 utilities)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from bashlint.data_tools import bash_parser, pretty_print, cmd2template\n",
        "from metric.metric_utils import compute_metric\n",
        "from functools import partial\n",
        "\n",
        "from collections import Counter\n",
        "import sentencepiece as spm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSfc8H8Z9b8S"
      },
      "source": [
        "Считаем данные. В столбце `invocation` находится текстовый запрос, в столбце `cmd` находится релевантная команда."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "NTKPnYFX9b8S",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "a6a70992-15ea-4466-f9b4-c1429394e48d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                          invocation  \\\n",
              "0  copy loadable kernel module \"mymodule.ko\" to t...   \n",
              "1  display all lines containing \"ip_mroute\" in th...   \n",
              "2  display current running kernel's compile-time ...   \n",
              "3  find all loadable modules for current kernel, ...   \n",
              "4  look for any instance of \"highmem\" in the curr...   \n",
              "\n",
              "                                                 cmd  \n",
              "0  sudo cp mymodule.ko /lib/modules/$(uname -r)/k...  \n",
              "1       cat /boot/config-`uname -r` | grep IP_MROUTE  \n",
              "2                        cat /boot/config-`uname -r`  \n",
              "3       find /lib/modules/`uname -r` -regex .*perf.*  \n",
              "4             grep “HIGHMEM” /boot/config-`uname -r`  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a2dffe5c-979e-46cf-bf67-fba8d2634fc0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>invocation</th>\n",
              "      <th>cmd</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>copy loadable kernel module \"mymodule.ko\" to t...</td>\n",
              "      <td>sudo cp mymodule.ko /lib/modules/$(uname -r)/k...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>display all lines containing \"ip_mroute\" in th...</td>\n",
              "      <td>cat /boot/config-`uname -r` | grep IP_MROUTE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>display current running kernel's compile-time ...</td>\n",
              "      <td>cat /boot/config-`uname -r`</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>find all loadable modules for current kernel, ...</td>\n",
              "      <td>find /lib/modules/`uname -r` -regex .*perf.*</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>look for any instance of \"highmem\" in the curr...</td>\n",
              "      <td>grep “HIGHMEM” /boot/config-`uname -r`</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a2dffe5c-979e-46cf-bf67-fba8d2634fc0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a2dffe5c-979e-46cf-bf67-fba8d2634fc0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a2dffe5c-979e-46cf-bf67-fba8d2634fc0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "train_data = pd.read_csv('data/train.csv')\n",
        "train_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNn9PKbr9b8S"
      },
      "source": [
        "В тестовых данных столбец `origin` отвечает за источник данных, значения `handrafted` соответствуют парам, составленными людьми, а `mined` парам, собранным автоматически."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "LBqTaxPH9b8S",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "65d62b36-4ac7-4d65-e121-6ec75d11b26f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                          invocation  \\\n",
              "0  create ssh connection to specified ip from spe...   \n",
              "1  search for commands containing string \"zeppeli...   \n",
              "2  search for location of specified file or appli...   \n",
              "3                    grant all rights to root folder   \n",
              "4     search in running processes for specified name   \n",
              "\n",
              "                        cmd       origin  \n",
              "0  ssh user123@176.0.13.154  handcrafted  \n",
              "1   history | grep zeppelin  handcrafted  \n",
              "2           whereis python3  handcrafted  \n",
              "3       sudo chmod 777 -R /  handcrafted  \n",
              "4       ps -aux | grep zepp  handcrafted  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a55df33a-2ef2-4a6d-a216-4b19c9f272cd\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>invocation</th>\n",
              "      <th>cmd</th>\n",
              "      <th>origin</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>create ssh connection to specified ip from spe...</td>\n",
              "      <td>ssh user123@176.0.13.154</td>\n",
              "      <td>handcrafted</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>search for commands containing string \"zeppeli...</td>\n",
              "      <td>history | grep zeppelin</td>\n",
              "      <td>handcrafted</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>search for location of specified file or appli...</td>\n",
              "      <td>whereis python3</td>\n",
              "      <td>handcrafted</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>grant all rights to root folder</td>\n",
              "      <td>sudo chmod 777 -R /</td>\n",
              "      <td>handcrafted</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>search in running processes for specified name</td>\n",
              "      <td>ps -aux | grep zepp</td>\n",
              "      <td>handcrafted</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a55df33a-2ef2-4a6d-a216-4b19c9f272cd')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a55df33a-2ef2-4a6d-a216-4b19c9f272cd button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a55df33a-2ef2-4a6d-a216-4b19c9f272cd');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "test_data = pd.read_csv('data/test.csv')\n",
        "test_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T22oD4ya9b8S"
      },
      "source": [
        "**Задание**. Проведите предобработку текста. Рекомендуется:\n",
        "* перевести всё в нижний регистр\n",
        "* удалить стоп-слова (специфичные для выборки)\n",
        "* провести стемминг токенов\n",
        "* удалить все символы кроме латинских букв"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "_DNRyj8z9b8S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80f0d1b8-7da0-4ecc-8a5b-3118c827cc33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "import re\n",
        "\n",
        "\n",
        "def clean_text(text):\n",
        "    ### YOUR CODE HERE ###\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    ps = PorterStemmer()\n",
        "    \n",
        "    text = text.lower()\n",
        "    text = re.sub(\"[^a-z ]+\", \"\", text)\n",
        "    words = filter(lambda x: x != '' and x not in stop_words, text.split())\n",
        "    words = list(map(lambda x: ps.stem(x), words))\n",
        "    return words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "erHtxhG49b8T"
      },
      "outputs": [],
      "source": [
        "train_data['text_cleaned'] = train_data['invocation'].apply(clean_text)\n",
        "test_data['text_cleaned'] = test_data['invocation'].apply(clean_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLrVq0ZS9b8T"
      },
      "source": [
        "Для обработки кода воспользуемся функцией `cmd2template`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "JLWJ47Pr9b8T"
      },
      "outputs": [],
      "source": [
        "train_data['cmd_cleaned'] = train_data['cmd'].apply(partial(cmd2template, loose_constraints=True))\n",
        "test_data['cmd_cleaned'] = test_data['cmd'].apply(partial(cmd2template, loose_constraints=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fnvHwek9b8T"
      },
      "source": [
        "Разделим данные на обучение и валидацию. Т.к. данных очень мало, то для валидационной выборки выделим только 100 примеров."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "UuKsjNuI9b8T"
      },
      "outputs": [],
      "source": [
        "valid_data = train_data.iloc[-100:]\n",
        "train_data = train_data.iloc[:-100]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKTDx1G49b8U"
      },
      "source": [
        "**Задание**. Стандартный формат входных данных для трансформеров — BPE токены. Воспользуйтесь библиотекой sentencepiece для обучения токенайзеров для текста и кода. Используйте небольшое число токенов."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "JEm7hhnS9b8U"
      },
      "outputs": [],
      "source": [
        "## YOUR CODE HERE ###\n",
        "with open('text', 'a', encoding='utf-8') as file:\n",
        "    for text in train_data['text_cleaned']:\n",
        "        file.write(' '.join(text) + '\\n')\n",
        "\n",
        "with open('cmd', 'a', encoding='utf-8') as file:\n",
        "    for text in train_data['cmd_cleaned']:\n",
        "        file.write(text + '\\n')\n",
        "\n",
        "text_trainer = spm.SentencePieceTrainer.train(input='text', model_prefix='text', model_type='bpe', vocab_size=3000)\n",
        "cmd_trainer = spm.SentencePieceTrainer.train(input='cmd', model_prefix='cmd', model_type='bpe', vocab_size=500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "iOdvh6lp9b8U"
      },
      "outputs": [],
      "source": [
        "text_tokenizer = spm.SentencePieceProcessor(model_file='text.model')  ## YOUR CODE HERE ###\n",
        "cmd_tokenizer = spm.SentencePieceProcessor(model_file='cmd.model') ## YOUR CODE HERE ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYpadHEq9b8U"
      },
      "source": [
        "**Задание**. Задайте датасеты и лоадеры для ваших данных. Каждая последовательность должна начинаться с BOS токена и заканчиваться EOS токеном. Рекомендуется ограничить длину входных и выходных последовательностей!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "HgP2jKK79b8U"
      },
      "outputs": [],
      "source": [
        "PAD_ID = 0\n",
        "BOS_ID = 1\n",
        "EOS_ID = 2\n",
        "\n",
        "\n",
        "MAX_TEXT_LENGTH = 256\n",
        "MAX_CODE_LENGTH = 40\n",
        "\n",
        "BATCH_SIZE = 16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "I-UV3RPL9b8U"
      },
      "outputs": [],
      "source": [
        "class TextToBashDataset(Dataset):\n",
        "    ## YOUR CODE HERE ###\n",
        "    def __init__(self, data_frame, text_tokenizer, cmd_tokenizer):\n",
        "        self._df = data_frame.reset_index(drop=True)\n",
        "        self._text_tokenizer = text_tokenizer\n",
        "        self._cmd_tokenizer = cmd_tokenizer\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self._df)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        text = ' '.join(self._df['text_cleaned'][idx])\n",
        "        cmd = self._df['cmd_cleaned'][idx]\n",
        "        text = torch.LongTensor(self._text_tokenizer.Encode(text, add_bos=True, add_eos=True))\n",
        "        cmd = torch.LongTensor(self._cmd_tokenizer.Encode(cmd, add_bos=True, add_eos=True))\n",
        "        return text[:MAX_TEXT_LENGTH], cmd[:MAX_CODE_LENGTH]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "K3HwluaM9b8V"
      },
      "outputs": [],
      "source": [
        "train_ds = TextToBashDataset(train_data, text_tokenizer, cmd_tokenizer) ## YOUR CODE HERE ###\n",
        "valid_ds = TextToBashDataset(valid_data, text_tokenizer, cmd_tokenizer) ## YOUR CODE HERE ###"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def pad(sequence):\n",
        "    max_length = max([el.shape[0] for el in sequence])\n",
        "    padded_batch = []\n",
        "    for el in sequence:\n",
        "        padded_batch.append(torch.cat([el, torch.LongTensor([PAD_ID] * (max_length - el.shape[0]))]))\n",
        "    return torch.stack(padded_batch)\n",
        "\n",
        "\n",
        "def collate(batch):\n",
        "    texts = [el[0] for el in batch]\n",
        "    cmds = [el[1] for el in batch]\n",
        "    return pad(texts), pad(cmds)\n"
      ],
      "metadata": {
        "id": "fzm3gDMUtLyx"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Ln7msKwt9b8V"
      },
      "outputs": [],
      "source": [
        "loaders = {\n",
        "    'train': DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate),\n",
        "    'valid': DataLoader(valid_ds, batch_size=BATCH_SIZE, collate_fn=collate),\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRVHKV9q9b8V"
      },
      "source": [
        "## Обучение бейзлайна (2 балла)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "ScHkfT5S9b8V"
      },
      "outputs": [],
      "source": [
        "from transformers import BertConfig, BertModel, EncoderDecoderConfig, EncoderDecoderModel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iugd3KFH9b8V"
      },
      "source": [
        "**Задание.** Реализуйте модель encoder-decoder ниже. В качестве моделей энкодера и декодера рекомендуется использовать BertModel из библиотеки transformers, заданную через BertConfig. В случае декодера необходимо выставить параметры is_decoder=True и add_cross_attention=True. В качестве модели, <<сцепляющей>> энкодер и декодер, в одну архитектуру рекомендуется использовать EncoderDecoderModel.\n",
        "\n",
        "**Обратите внимание!** EncoderDecoderModel поддерживает использование кэшированных результатов при последовательной генерации. Это пригодится при реализации beam-search ниже.\n",
        "\n",
        "Для того, чтобы удобнее задавать модели, рекомендуется реализовать задание модели через конфиг. Ниже представлены базовые параметры, при которых модель должна работать быстро и с приемлемым качеством."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "8S6m3mDu9b8V"
      },
      "outputs": [],
      "source": [
        "text_model_config = {\n",
        "    'vocab': text_tokenizer.vocab_size(),\n",
        "    'hidden_size': 256,\n",
        "    'num_layers': 2,\n",
        "    'num_attention_heads': 8,\n",
        "    'intermediate_size': 256 * 4,\n",
        "    'hidden_dropout_prob': 0.1,\n",
        "    'pad_id': PAD_ID,\n",
        "}\n",
        "\n",
        "cmd_model_config = {\n",
        "    'vocab': cmd_tokenizer.vocab_size(),\n",
        "    'hidden_size': 256,\n",
        "    'num_layers': 2,\n",
        "    'num_attention_heads': 8,\n",
        "    'intermediate_size': 256 * 4,\n",
        "    'hidden_dropout_prob': 0.1,\n",
        "    'pad_id': PAD_ID,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Xsv162PK9b8W"
      },
      "outputs": [],
      "source": [
        "class TextToBashModel(nn.Module):\n",
        "    def __init__(self, text_model_config, cmd_model_config):\n",
        "        super(TextToBashModel, self).__init__()\n",
        "        ## YOUR CODE HERE ##\n",
        "        encoder_config = BertConfig(**text_model_config)\n",
        "        cmd_model_config['is_decoder'] = True\n",
        "        cmd_model_config['add_cross_attention'] = True\n",
        "        decoder_config = BertConfig(**cmd_model_config)\n",
        "        encoder_decoder_config = EncoderDecoderConfig.from_encoder_decoder_configs(encoder_config, decoder_config)\n",
        "        config = EncoderDecoderConfig.from_encoder_decoder_configs(encoder_config, decoder_config)        \n",
        "        self._coupling = EncoderDecoderModel(encoder_decoder_config)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        ## YOUR CODE HERE ##\n",
        "        text_ids, cmd_ids = x\n",
        "        mask = (text_ids != PAD_ID).float()\n",
        "        return self._coupling(input_ids=text_ids, decoder_input_ids=cmd_ids[:, :-1], attention_mask=mask)        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znbpWiqX9b8W"
      },
      "source": [
        "**Задание**. Обучите вашу модель ниже.\n",
        "\n",
        "Рекомендуется:\n",
        "* в качестве лосса использовать стандартную кросс-энтропию, не забывайте игнорировать PAD токены\n",
        "* использовать Adam для оптимизации\n",
        "* не использовать scheduler для бейзлайна (модель легко переобучается с ним)\n",
        "* использовать early stopping по валидационному лоссу"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "ZIfOylR79b8W"
      },
      "outputs": [],
      "source": [
        "## YOUR CODE HERE ##\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from  torch.nn.utils import clip_grad_norm_\n",
        "\n",
        "class Trainer:\n",
        "\n",
        "    def __init__(\n",
        "            self, \n",
        "            model,\n",
        "            optimizer, \n",
        "            pad_token_id,\n",
        "            device,\n",
        "            logdir='bert-text2bash',\n",
        "            max_grad_norm=1\n",
        "    ):\n",
        "        self._device = device\n",
        "        self._model = model.to(device)\n",
        "        self._optimizer = optimizer\n",
        "        self._pad_token_id = pad_token_id\n",
        "        self._criterion = nn.CrossEntropyLoss()\n",
        "        self._max_grad_norm = max_grad_norm\n",
        "        \n",
        "        self._writer = SummaryWriter(log_dir=logdir)\n",
        "        self._step = 0\n",
        "        self._n_epoch = 0\n",
        "        \n",
        "\n",
        "    def train(self, dataloaders, n_epochs):\n",
        "        for epoch in range(n_epochs):\n",
        "            self._train_step(dataloaders)\n",
        "            self._n_epoch += 1\n",
        "            \n",
        "    def _validate(self, dataloader):\n",
        "        total_loss = 0\n",
        "        for texts, cmds in dataloader:\n",
        "            texts = texts.to(self._device)\n",
        "            cmds = cmds.to(self._device)\n",
        "            with torch.no_grad():\n",
        "                decoded = self._model((texts, cmds)).logits\n",
        "            decoded = decoded.view(-1, decoded.shape[-1])\n",
        "            cmds = cmds[:, 1:]\n",
        "            cmds = cmds.reshape(-1)\n",
        "            indicies = (cmds != PAD_ID)\n",
        "            loss = self._criterion(decoded[indicies], cmds[indicies])\n",
        "            total_loss += loss.item()\n",
        "        return total_loss / len(dataloader)\n",
        "\n",
        "    def _train_step(self, dataloaders):\n",
        "        for i, (texts, cmds) in enumerate(dataloaders['train']):\n",
        "            texts = texts.to(self._device)\n",
        "            cmds = cmds.to(self._device)\n",
        "            model_output = self._model((texts, cmds))\n",
        "            decoded = model_output.logits\n",
        "            decoded = decoded.reshape(-1, decoded.shape[-1])\n",
        "            cmds = cmds[:, 1:]\n",
        "            cmds = cmds.reshape(-1)\n",
        "            indicies = (cmds != PAD_ID)\n",
        "            loss = self._criterion(decoded[indicies], cmds[indicies])\n",
        "            self._writer.add_scalar('Loss/train', loss.item(), self._step)\n",
        "            loss.backward()\n",
        "            clip_grad_norm_(self._model.parameters(), self._max_grad_norm)\n",
        "            if i % 4 == 0:\n",
        "                self._optimizer.step()\n",
        "                self._optimizer.zero_grad()\n",
        "            if i % 20 == 0:\n",
        "                val_loss = self._validate(dataloaders['valid'])\n",
        "                self._writer.add_scalar('Loss/valid', val_loss, self._step)\n",
        "                print(f'epoch {self._n_epoch} iter {i} val_loss: {val_loss:.4}')\n",
        "            self._step += 1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = TextToBashModel(text_model_config, cmd_model_config)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "device = 'cuda'\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.ipc_collect()\n",
        "trainer = Trainer(model, optimizer, PAD_ID, device)\n",
        "trainer.train(loaders, 28)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EvZTyI8Htzz9",
        "outputId": "904401e4-dbfa-452c-db44-2304cb9d5bfc"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0 iter 0 val_loss: 10.0\n",
            "epoch 0 iter 20 val_loss: 9.328\n",
            "epoch 0 iter 40 val_loss: 8.998\n",
            "epoch 0 iter 60 val_loss: 8.748\n",
            "epoch 0 iter 80 val_loss: 8.517\n",
            "epoch 0 iter 100 val_loss: 8.274\n",
            "epoch 0 iter 120 val_loss: 8.063\n",
            "epoch 0 iter 140 val_loss: 7.864\n",
            "epoch 0 iter 160 val_loss: 7.658\n",
            "epoch 0 iter 180 val_loss: 7.458\n",
            "epoch 0 iter 200 val_loss: 7.237\n",
            "epoch 0 iter 220 val_loss: 7.029\n",
            "epoch 0 iter 240 val_loss: 6.807\n",
            "epoch 0 iter 260 val_loss: 6.606\n",
            "epoch 0 iter 280 val_loss: 6.401\n",
            "epoch 0 iter 300 val_loss: 6.196\n",
            "epoch 0 iter 320 val_loss: 6.004\n",
            "epoch 0 iter 340 val_loss: 5.811\n",
            "epoch 0 iter 360 val_loss: 5.62\n",
            "epoch 0 iter 380 val_loss: 5.432\n",
            "epoch 0 iter 400 val_loss: 5.249\n",
            "epoch 0 iter 420 val_loss: 5.078\n",
            "epoch 0 iter 440 val_loss: 4.923\n",
            "epoch 0 iter 460 val_loss: 4.764\n",
            "epoch 0 iter 480 val_loss: 4.627\n",
            "epoch 0 iter 500 val_loss: 4.485\n",
            "epoch 0 iter 520 val_loss: 4.384\n",
            "epoch 0 iter 540 val_loss: 4.262\n",
            "epoch 0 iter 560 val_loss: 4.148\n",
            "epoch 0 iter 580 val_loss: 4.046\n",
            "epoch 0 iter 600 val_loss: 3.967\n",
            "epoch 1 iter 0 val_loss: 3.897\n",
            "epoch 1 iter 20 val_loss: 3.823\n",
            "epoch 1 iter 40 val_loss: 3.774\n",
            "epoch 1 iter 60 val_loss: 3.712\n",
            "epoch 1 iter 80 val_loss: 3.664\n",
            "epoch 1 iter 100 val_loss: 3.601\n",
            "epoch 1 iter 120 val_loss: 3.564\n",
            "epoch 1 iter 140 val_loss: 3.537\n",
            "epoch 1 iter 160 val_loss: 3.478\n",
            "epoch 1 iter 180 val_loss: 3.455\n",
            "epoch 1 iter 200 val_loss: 3.406\n",
            "epoch 1 iter 220 val_loss: 3.357\n",
            "epoch 1 iter 240 val_loss: 3.311\n",
            "epoch 1 iter 260 val_loss: 3.268\n",
            "epoch 1 iter 280 val_loss: 3.248\n",
            "epoch 1 iter 300 val_loss: 3.22\n",
            "epoch 1 iter 320 val_loss: 3.202\n",
            "epoch 1 iter 340 val_loss: 3.177\n",
            "epoch 1 iter 360 val_loss: 3.136\n",
            "epoch 1 iter 380 val_loss: 3.103\n",
            "epoch 1 iter 400 val_loss: 3.08\n",
            "epoch 1 iter 420 val_loss: 3.049\n",
            "epoch 1 iter 440 val_loss: 3.033\n",
            "epoch 1 iter 460 val_loss: 3.003\n",
            "epoch 1 iter 480 val_loss: 3.014\n",
            "epoch 1 iter 500 val_loss: 2.958\n",
            "epoch 1 iter 520 val_loss: 2.95\n",
            "epoch 1 iter 540 val_loss: 2.944\n",
            "epoch 1 iter 560 val_loss: 2.942\n",
            "epoch 1 iter 580 val_loss: 2.908\n",
            "epoch 1 iter 600 val_loss: 2.888\n",
            "epoch 2 iter 0 val_loss: 2.865\n",
            "epoch 2 iter 20 val_loss: 2.87\n",
            "epoch 2 iter 40 val_loss: 2.85\n",
            "epoch 2 iter 60 val_loss: 2.809\n",
            "epoch 2 iter 80 val_loss: 2.81\n",
            "epoch 2 iter 100 val_loss: 2.794\n",
            "epoch 2 iter 120 val_loss: 2.759\n",
            "epoch 2 iter 140 val_loss: 2.754\n",
            "epoch 2 iter 160 val_loss: 2.73\n",
            "epoch 2 iter 180 val_loss: 2.733\n",
            "epoch 2 iter 200 val_loss: 2.733\n",
            "epoch 2 iter 220 val_loss: 2.701\n",
            "epoch 2 iter 240 val_loss: 2.71\n",
            "epoch 2 iter 260 val_loss: 2.691\n",
            "epoch 2 iter 280 val_loss: 2.7\n",
            "epoch 2 iter 300 val_loss: 2.652\n",
            "epoch 2 iter 320 val_loss: 2.637\n",
            "epoch 2 iter 340 val_loss: 2.631\n",
            "epoch 2 iter 360 val_loss: 2.597\n",
            "epoch 2 iter 380 val_loss: 2.598\n",
            "epoch 2 iter 400 val_loss: 2.607\n",
            "epoch 2 iter 420 val_loss: 2.565\n",
            "epoch 2 iter 440 val_loss: 2.564\n",
            "epoch 2 iter 460 val_loss: 2.568\n",
            "epoch 2 iter 480 val_loss: 2.535\n",
            "epoch 2 iter 500 val_loss: 2.551\n",
            "epoch 2 iter 520 val_loss: 2.515\n",
            "epoch 2 iter 540 val_loss: 2.529\n",
            "epoch 2 iter 560 val_loss: 2.506\n",
            "epoch 2 iter 580 val_loss: 2.468\n",
            "epoch 2 iter 600 val_loss: 2.465\n",
            "epoch 3 iter 0 val_loss: 2.463\n",
            "epoch 3 iter 20 val_loss: 2.451\n",
            "epoch 3 iter 40 val_loss: 2.478\n",
            "epoch 3 iter 60 val_loss: 2.437\n",
            "epoch 3 iter 80 val_loss: 2.444\n",
            "epoch 3 iter 100 val_loss: 2.47\n",
            "epoch 3 iter 120 val_loss: 2.459\n",
            "epoch 3 iter 140 val_loss: 2.408\n",
            "epoch 3 iter 160 val_loss: 2.42\n",
            "epoch 3 iter 180 val_loss: 2.394\n",
            "epoch 3 iter 200 val_loss: 2.381\n",
            "epoch 3 iter 220 val_loss: 2.349\n",
            "epoch 3 iter 240 val_loss: 2.391\n",
            "epoch 3 iter 260 val_loss: 2.374\n",
            "epoch 3 iter 280 val_loss: 2.36\n",
            "epoch 3 iter 300 val_loss: 2.367\n",
            "epoch 3 iter 320 val_loss: 2.346\n",
            "epoch 3 iter 340 val_loss: 2.34\n",
            "epoch 3 iter 360 val_loss: 2.34\n",
            "epoch 3 iter 380 val_loss: 2.317\n",
            "epoch 3 iter 400 val_loss: 2.324\n",
            "epoch 3 iter 420 val_loss: 2.311\n",
            "epoch 3 iter 440 val_loss: 2.34\n",
            "epoch 3 iter 460 val_loss: 2.334\n",
            "epoch 3 iter 480 val_loss: 2.284\n",
            "epoch 3 iter 500 val_loss: 2.27\n",
            "epoch 3 iter 520 val_loss: 2.302\n",
            "epoch 3 iter 540 val_loss: 2.257\n",
            "epoch 3 iter 560 val_loss: 2.254\n",
            "epoch 3 iter 580 val_loss: 2.271\n",
            "epoch 3 iter 600 val_loss: 2.235\n",
            "epoch 4 iter 0 val_loss: 2.239\n",
            "epoch 4 iter 20 val_loss: 2.225\n",
            "epoch 4 iter 40 val_loss: 2.199\n",
            "epoch 4 iter 60 val_loss: 2.203\n",
            "epoch 4 iter 80 val_loss: 2.186\n",
            "epoch 4 iter 100 val_loss: 2.21\n",
            "epoch 4 iter 120 val_loss: 2.196\n",
            "epoch 4 iter 140 val_loss: 2.202\n",
            "epoch 4 iter 160 val_loss: 2.184\n",
            "epoch 4 iter 180 val_loss: 2.183\n",
            "epoch 4 iter 200 val_loss: 2.189\n",
            "epoch 4 iter 220 val_loss: 2.172\n",
            "epoch 4 iter 240 val_loss: 2.193\n",
            "epoch 4 iter 260 val_loss: 2.156\n",
            "epoch 4 iter 280 val_loss: 2.156\n",
            "epoch 4 iter 300 val_loss: 2.141\n",
            "epoch 4 iter 320 val_loss: 2.143\n",
            "epoch 4 iter 340 val_loss: 2.138\n",
            "epoch 4 iter 360 val_loss: 2.163\n",
            "epoch 4 iter 380 val_loss: 2.147\n",
            "epoch 4 iter 400 val_loss: 2.142\n",
            "epoch 4 iter 420 val_loss: 2.154\n",
            "epoch 4 iter 440 val_loss: 2.127\n",
            "epoch 4 iter 460 val_loss: 2.187\n",
            "epoch 4 iter 480 val_loss: 2.128\n",
            "epoch 4 iter 500 val_loss: 2.133\n",
            "epoch 4 iter 520 val_loss: 2.136\n",
            "epoch 4 iter 540 val_loss: 2.093\n",
            "epoch 4 iter 560 val_loss: 2.11\n",
            "epoch 4 iter 580 val_loss: 2.102\n",
            "epoch 4 iter 600 val_loss: 2.113\n",
            "epoch 5 iter 0 val_loss: 2.127\n",
            "epoch 5 iter 20 val_loss: 2.107\n",
            "epoch 5 iter 40 val_loss: 2.086\n",
            "epoch 5 iter 60 val_loss: 2.076\n",
            "epoch 5 iter 80 val_loss: 2.086\n",
            "epoch 5 iter 100 val_loss: 2.053\n",
            "epoch 5 iter 120 val_loss: 2.063\n",
            "epoch 5 iter 140 val_loss: 2.078\n",
            "epoch 5 iter 160 val_loss: 2.064\n",
            "epoch 5 iter 180 val_loss: 2.031\n",
            "epoch 5 iter 200 val_loss: 2.045\n",
            "epoch 5 iter 220 val_loss: 2.031\n",
            "epoch 5 iter 240 val_loss: 2.026\n",
            "epoch 5 iter 260 val_loss: 2.017\n",
            "epoch 5 iter 280 val_loss: 2.038\n",
            "epoch 5 iter 300 val_loss: 2.063\n",
            "epoch 5 iter 320 val_loss: 2.024\n",
            "epoch 5 iter 340 val_loss: 2.02\n",
            "epoch 5 iter 360 val_loss: 2.005\n",
            "epoch 5 iter 380 val_loss: 2.011\n",
            "epoch 5 iter 400 val_loss: 2.005\n",
            "epoch 5 iter 420 val_loss: 1.99\n",
            "epoch 5 iter 440 val_loss: 2.005\n",
            "epoch 5 iter 460 val_loss: 1.958\n",
            "epoch 5 iter 480 val_loss: 1.989\n",
            "epoch 5 iter 500 val_loss: 2.003\n",
            "epoch 5 iter 520 val_loss: 1.986\n",
            "epoch 5 iter 540 val_loss: 1.993\n",
            "epoch 5 iter 560 val_loss: 1.992\n",
            "epoch 5 iter 580 val_loss: 1.965\n",
            "epoch 5 iter 600 val_loss: 1.956\n",
            "epoch 6 iter 0 val_loss: 1.972\n",
            "epoch 6 iter 20 val_loss: 1.963\n",
            "epoch 6 iter 40 val_loss: 1.919\n",
            "epoch 6 iter 60 val_loss: 1.944\n",
            "epoch 6 iter 80 val_loss: 1.928\n",
            "epoch 6 iter 100 val_loss: 1.917\n",
            "epoch 6 iter 120 val_loss: 1.932\n",
            "epoch 6 iter 140 val_loss: 1.901\n",
            "epoch 6 iter 160 val_loss: 1.91\n",
            "epoch 6 iter 180 val_loss: 1.932\n",
            "epoch 6 iter 200 val_loss: 1.919\n",
            "epoch 6 iter 220 val_loss: 1.901\n",
            "epoch 6 iter 240 val_loss: 1.885\n",
            "epoch 6 iter 260 val_loss: 1.893\n",
            "epoch 6 iter 280 val_loss: 1.925\n",
            "epoch 6 iter 300 val_loss: 1.942\n",
            "epoch 6 iter 320 val_loss: 1.908\n",
            "epoch 6 iter 340 val_loss: 1.895\n",
            "epoch 6 iter 360 val_loss: 1.899\n",
            "epoch 6 iter 380 val_loss: 1.908\n",
            "epoch 6 iter 400 val_loss: 1.913\n",
            "epoch 6 iter 420 val_loss: 1.909\n",
            "epoch 6 iter 440 val_loss: 1.894\n",
            "epoch 6 iter 460 val_loss: 1.883\n",
            "epoch 6 iter 480 val_loss: 1.883\n",
            "epoch 6 iter 500 val_loss: 1.902\n",
            "epoch 6 iter 520 val_loss: 1.913\n",
            "epoch 6 iter 540 val_loss: 1.898\n",
            "epoch 6 iter 560 val_loss: 1.892\n",
            "epoch 6 iter 580 val_loss: 1.851\n",
            "epoch 6 iter 600 val_loss: 1.87\n",
            "epoch 7 iter 0 val_loss: 1.877\n",
            "epoch 7 iter 20 val_loss: 1.884\n",
            "epoch 7 iter 40 val_loss: 1.881\n",
            "epoch 7 iter 60 val_loss: 1.861\n",
            "epoch 7 iter 80 val_loss: 1.873\n",
            "epoch 7 iter 100 val_loss: 1.88\n",
            "epoch 7 iter 120 val_loss: 1.828\n",
            "epoch 7 iter 140 val_loss: 1.858\n",
            "epoch 7 iter 160 val_loss: 1.847\n",
            "epoch 7 iter 180 val_loss: 1.85\n",
            "epoch 7 iter 200 val_loss: 1.838\n",
            "epoch 7 iter 220 val_loss: 1.874\n",
            "epoch 7 iter 240 val_loss: 1.854\n",
            "epoch 7 iter 260 val_loss: 1.857\n",
            "epoch 7 iter 280 val_loss: 1.84\n",
            "epoch 7 iter 300 val_loss: 1.834\n",
            "epoch 7 iter 320 val_loss: 1.807\n",
            "epoch 7 iter 340 val_loss: 1.808\n",
            "epoch 7 iter 360 val_loss: 1.825\n",
            "epoch 7 iter 380 val_loss: 1.815\n",
            "epoch 7 iter 400 val_loss: 1.8\n",
            "epoch 7 iter 420 val_loss: 1.788\n",
            "epoch 7 iter 440 val_loss: 1.826\n",
            "epoch 7 iter 460 val_loss: 1.808\n",
            "epoch 7 iter 480 val_loss: 1.794\n",
            "epoch 7 iter 500 val_loss: 1.794\n",
            "epoch 7 iter 520 val_loss: 1.804\n",
            "epoch 7 iter 540 val_loss: 1.807\n",
            "epoch 7 iter 560 val_loss: 1.785\n",
            "epoch 7 iter 580 val_loss: 1.793\n",
            "epoch 7 iter 600 val_loss: 1.76\n",
            "epoch 8 iter 0 val_loss: 1.786\n",
            "epoch 8 iter 20 val_loss: 1.769\n",
            "epoch 8 iter 40 val_loss: 1.785\n",
            "epoch 8 iter 60 val_loss: 1.767\n",
            "epoch 8 iter 80 val_loss: 1.729\n",
            "epoch 8 iter 100 val_loss: 1.731\n",
            "epoch 8 iter 120 val_loss: 1.753\n",
            "epoch 8 iter 140 val_loss: 1.746\n",
            "epoch 8 iter 160 val_loss: 1.76\n",
            "epoch 8 iter 180 val_loss: 1.722\n",
            "epoch 8 iter 200 val_loss: 1.75\n",
            "epoch 8 iter 220 val_loss: 1.754\n",
            "epoch 8 iter 240 val_loss: 1.772\n",
            "epoch 8 iter 260 val_loss: 1.747\n",
            "epoch 8 iter 280 val_loss: 1.751\n",
            "epoch 8 iter 300 val_loss: 1.736\n",
            "epoch 8 iter 320 val_loss: 1.716\n",
            "epoch 8 iter 340 val_loss: 1.74\n",
            "epoch 8 iter 360 val_loss: 1.73\n",
            "epoch 8 iter 380 val_loss: 1.758\n",
            "epoch 8 iter 400 val_loss: 1.755\n",
            "epoch 8 iter 420 val_loss: 1.746\n",
            "epoch 8 iter 440 val_loss: 1.741\n",
            "epoch 8 iter 460 val_loss: 1.709\n",
            "epoch 8 iter 480 val_loss: 1.702\n",
            "epoch 8 iter 500 val_loss: 1.702\n",
            "epoch 8 iter 520 val_loss: 1.684\n",
            "epoch 8 iter 540 val_loss: 1.693\n",
            "epoch 8 iter 560 val_loss: 1.7\n",
            "epoch 8 iter 580 val_loss: 1.702\n",
            "epoch 8 iter 600 val_loss: 1.724\n",
            "epoch 9 iter 0 val_loss: 1.734\n",
            "epoch 9 iter 20 val_loss: 1.703\n",
            "epoch 9 iter 40 val_loss: 1.742\n",
            "epoch 9 iter 60 val_loss: 1.727\n",
            "epoch 9 iter 80 val_loss: 1.731\n",
            "epoch 9 iter 100 val_loss: 1.714\n",
            "epoch 9 iter 120 val_loss: 1.707\n",
            "epoch 9 iter 140 val_loss: 1.693\n",
            "epoch 9 iter 160 val_loss: 1.72\n",
            "epoch 9 iter 180 val_loss: 1.705\n",
            "epoch 9 iter 200 val_loss: 1.685\n",
            "epoch 9 iter 220 val_loss: 1.688\n",
            "epoch 9 iter 240 val_loss: 1.694\n",
            "epoch 9 iter 260 val_loss: 1.718\n",
            "epoch 9 iter 280 val_loss: 1.69\n",
            "epoch 9 iter 300 val_loss: 1.69\n",
            "epoch 9 iter 320 val_loss: 1.707\n",
            "epoch 9 iter 340 val_loss: 1.701\n",
            "epoch 9 iter 360 val_loss: 1.735\n",
            "epoch 9 iter 380 val_loss: 1.732\n",
            "epoch 9 iter 400 val_loss: 1.7\n",
            "epoch 9 iter 420 val_loss: 1.68\n",
            "epoch 9 iter 440 val_loss: 1.672\n",
            "epoch 9 iter 460 val_loss: 1.691\n",
            "epoch 9 iter 480 val_loss: 1.695\n",
            "epoch 9 iter 500 val_loss: 1.694\n",
            "epoch 9 iter 520 val_loss: 1.683\n",
            "epoch 9 iter 540 val_loss: 1.671\n",
            "epoch 9 iter 560 val_loss: 1.671\n",
            "epoch 9 iter 580 val_loss: 1.694\n",
            "epoch 9 iter 600 val_loss: 1.671\n",
            "epoch 10 iter 0 val_loss: 1.692\n",
            "epoch 10 iter 20 val_loss: 1.662\n",
            "epoch 10 iter 40 val_loss: 1.675\n",
            "epoch 10 iter 60 val_loss: 1.681\n",
            "epoch 10 iter 80 val_loss: 1.673\n",
            "epoch 10 iter 100 val_loss: 1.678\n",
            "epoch 10 iter 120 val_loss: 1.661\n",
            "epoch 10 iter 140 val_loss: 1.664\n",
            "epoch 10 iter 160 val_loss: 1.662\n",
            "epoch 10 iter 180 val_loss: 1.669\n",
            "epoch 10 iter 200 val_loss: 1.67\n",
            "epoch 10 iter 220 val_loss: 1.671\n",
            "epoch 10 iter 240 val_loss: 1.653\n",
            "epoch 10 iter 260 val_loss: 1.641\n",
            "epoch 10 iter 280 val_loss: 1.599\n",
            "epoch 10 iter 300 val_loss: 1.631\n",
            "epoch 10 iter 320 val_loss: 1.653\n",
            "epoch 10 iter 340 val_loss: 1.661\n",
            "epoch 10 iter 360 val_loss: 1.668\n",
            "epoch 10 iter 380 val_loss: 1.674\n",
            "epoch 10 iter 400 val_loss: 1.662\n",
            "epoch 10 iter 420 val_loss: 1.66\n",
            "epoch 10 iter 440 val_loss: 1.647\n",
            "epoch 10 iter 460 val_loss: 1.662\n",
            "epoch 10 iter 480 val_loss: 1.672\n",
            "epoch 10 iter 500 val_loss: 1.651\n",
            "epoch 10 iter 520 val_loss: 1.586\n",
            "epoch 10 iter 540 val_loss: 1.634\n",
            "epoch 10 iter 560 val_loss: 1.631\n",
            "epoch 10 iter 580 val_loss: 1.638\n",
            "epoch 10 iter 600 val_loss: 1.636\n",
            "epoch 11 iter 0 val_loss: 1.656\n",
            "epoch 11 iter 20 val_loss: 1.622\n",
            "epoch 11 iter 40 val_loss: 1.617\n",
            "epoch 11 iter 60 val_loss: 1.607\n",
            "epoch 11 iter 80 val_loss: 1.611\n",
            "epoch 11 iter 100 val_loss: 1.639\n",
            "epoch 11 iter 120 val_loss: 1.651\n",
            "epoch 11 iter 140 val_loss: 1.662\n",
            "epoch 11 iter 160 val_loss: 1.637\n",
            "epoch 11 iter 180 val_loss: 1.643\n",
            "epoch 11 iter 200 val_loss: 1.629\n",
            "epoch 11 iter 220 val_loss: 1.62\n",
            "epoch 11 iter 240 val_loss: 1.616\n",
            "epoch 11 iter 260 val_loss: 1.622\n",
            "epoch 11 iter 280 val_loss: 1.628\n",
            "epoch 11 iter 300 val_loss: 1.649\n",
            "epoch 11 iter 320 val_loss: 1.629\n",
            "epoch 11 iter 340 val_loss: 1.626\n",
            "epoch 11 iter 360 val_loss: 1.652\n",
            "epoch 11 iter 380 val_loss: 1.607\n",
            "epoch 11 iter 400 val_loss: 1.614\n",
            "epoch 11 iter 420 val_loss: 1.594\n",
            "epoch 11 iter 440 val_loss: 1.599\n",
            "epoch 11 iter 460 val_loss: 1.595\n",
            "epoch 11 iter 480 val_loss: 1.595\n",
            "epoch 11 iter 500 val_loss: 1.589\n",
            "epoch 11 iter 520 val_loss: 1.582\n",
            "epoch 11 iter 540 val_loss: 1.6\n",
            "epoch 11 iter 560 val_loss: 1.565\n",
            "epoch 11 iter 580 val_loss: 1.589\n",
            "epoch 11 iter 600 val_loss: 1.58\n",
            "epoch 12 iter 0 val_loss: 1.587\n",
            "epoch 12 iter 20 val_loss: 1.603\n",
            "epoch 12 iter 40 val_loss: 1.596\n",
            "epoch 12 iter 60 val_loss: 1.59\n",
            "epoch 12 iter 80 val_loss: 1.565\n",
            "epoch 12 iter 100 val_loss: 1.561\n",
            "epoch 12 iter 120 val_loss: 1.586\n",
            "epoch 12 iter 140 val_loss: 1.587\n",
            "epoch 12 iter 160 val_loss: 1.581\n",
            "epoch 12 iter 180 val_loss: 1.587\n",
            "epoch 12 iter 200 val_loss: 1.553\n",
            "epoch 12 iter 220 val_loss: 1.575\n",
            "epoch 12 iter 240 val_loss: 1.593\n",
            "epoch 12 iter 260 val_loss: 1.566\n",
            "epoch 12 iter 280 val_loss: 1.562\n",
            "epoch 12 iter 300 val_loss: 1.543\n",
            "epoch 12 iter 320 val_loss: 1.551\n",
            "epoch 12 iter 340 val_loss: 1.553\n",
            "epoch 12 iter 360 val_loss: 1.591\n",
            "epoch 12 iter 380 val_loss: 1.573\n",
            "epoch 12 iter 400 val_loss: 1.586\n",
            "epoch 12 iter 420 val_loss: 1.565\n",
            "epoch 12 iter 440 val_loss: 1.572\n",
            "epoch 12 iter 460 val_loss: 1.568\n",
            "epoch 12 iter 480 val_loss: 1.543\n",
            "epoch 12 iter 500 val_loss: 1.532\n",
            "epoch 12 iter 520 val_loss: 1.551\n",
            "epoch 12 iter 540 val_loss: 1.579\n",
            "epoch 12 iter 560 val_loss: 1.548\n",
            "epoch 12 iter 580 val_loss: 1.52\n",
            "epoch 12 iter 600 val_loss: 1.503\n",
            "epoch 13 iter 0 val_loss: 1.503\n",
            "epoch 13 iter 20 val_loss: 1.523\n",
            "epoch 13 iter 40 val_loss: 1.527\n",
            "epoch 13 iter 60 val_loss: 1.502\n",
            "epoch 13 iter 80 val_loss: 1.526\n",
            "epoch 13 iter 100 val_loss: 1.518\n",
            "epoch 13 iter 120 val_loss: 1.508\n",
            "epoch 13 iter 140 val_loss: 1.488\n",
            "epoch 13 iter 160 val_loss: 1.528\n",
            "epoch 13 iter 180 val_loss: 1.505\n",
            "epoch 13 iter 200 val_loss: 1.5\n",
            "epoch 13 iter 220 val_loss: 1.506\n",
            "epoch 13 iter 240 val_loss: 1.533\n",
            "epoch 13 iter 260 val_loss: 1.547\n",
            "epoch 13 iter 280 val_loss: 1.531\n",
            "epoch 13 iter 300 val_loss: 1.52\n",
            "epoch 13 iter 320 val_loss: 1.513\n",
            "epoch 13 iter 340 val_loss: 1.534\n",
            "epoch 13 iter 360 val_loss: 1.523\n",
            "epoch 13 iter 380 val_loss: 1.536\n",
            "epoch 13 iter 400 val_loss: 1.555\n",
            "epoch 13 iter 420 val_loss: 1.531\n",
            "epoch 13 iter 440 val_loss: 1.524\n",
            "epoch 13 iter 460 val_loss: 1.521\n",
            "epoch 13 iter 480 val_loss: 1.54\n",
            "epoch 13 iter 500 val_loss: 1.533\n",
            "epoch 13 iter 520 val_loss: 1.537\n",
            "epoch 13 iter 540 val_loss: 1.543\n",
            "epoch 13 iter 560 val_loss: 1.544\n",
            "epoch 13 iter 580 val_loss: 1.547\n",
            "epoch 13 iter 600 val_loss: 1.556\n",
            "epoch 14 iter 0 val_loss: 1.54\n",
            "epoch 14 iter 20 val_loss: 1.54\n",
            "epoch 14 iter 40 val_loss: 1.535\n",
            "epoch 14 iter 60 val_loss: 1.529\n",
            "epoch 14 iter 80 val_loss: 1.535\n",
            "epoch 14 iter 100 val_loss: 1.547\n",
            "epoch 14 iter 120 val_loss: 1.555\n",
            "epoch 14 iter 140 val_loss: 1.525\n",
            "epoch 14 iter 160 val_loss: 1.554\n",
            "epoch 14 iter 180 val_loss: 1.548\n",
            "epoch 14 iter 200 val_loss: 1.535\n",
            "epoch 14 iter 220 val_loss: 1.532\n",
            "epoch 14 iter 240 val_loss: 1.521\n",
            "epoch 14 iter 260 val_loss: 1.553\n",
            "epoch 14 iter 280 val_loss: 1.556\n",
            "epoch 14 iter 300 val_loss: 1.539\n",
            "epoch 14 iter 320 val_loss: 1.535\n",
            "epoch 14 iter 340 val_loss: 1.5\n",
            "epoch 14 iter 360 val_loss: 1.479\n",
            "epoch 14 iter 380 val_loss: 1.48\n",
            "epoch 14 iter 400 val_loss: 1.506\n",
            "epoch 14 iter 420 val_loss: 1.529\n",
            "epoch 14 iter 440 val_loss: 1.565\n",
            "epoch 14 iter 460 val_loss: 1.491\n",
            "epoch 14 iter 480 val_loss: 1.521\n",
            "epoch 14 iter 500 val_loss: 1.512\n",
            "epoch 14 iter 520 val_loss: 1.536\n",
            "epoch 14 iter 540 val_loss: 1.516\n",
            "epoch 14 iter 560 val_loss: 1.477\n",
            "epoch 14 iter 580 val_loss: 1.49\n",
            "epoch 14 iter 600 val_loss: 1.478\n",
            "epoch 15 iter 0 val_loss: 1.506\n",
            "epoch 15 iter 20 val_loss: 1.494\n",
            "epoch 15 iter 40 val_loss: 1.505\n",
            "epoch 15 iter 60 val_loss: 1.509\n",
            "epoch 15 iter 80 val_loss: 1.521\n",
            "epoch 15 iter 100 val_loss: 1.506\n",
            "epoch 15 iter 120 val_loss: 1.514\n",
            "epoch 15 iter 140 val_loss: 1.487\n",
            "epoch 15 iter 160 val_loss: 1.491\n",
            "epoch 15 iter 180 val_loss: 1.472\n",
            "epoch 15 iter 200 val_loss: 1.496\n",
            "epoch 15 iter 220 val_loss: 1.537\n",
            "epoch 15 iter 240 val_loss: 1.531\n",
            "epoch 15 iter 260 val_loss: 1.496\n",
            "epoch 15 iter 280 val_loss: 1.503\n",
            "epoch 15 iter 300 val_loss: 1.473\n",
            "epoch 15 iter 320 val_loss: 1.518\n",
            "epoch 15 iter 340 val_loss: 1.494\n",
            "epoch 15 iter 360 val_loss: 1.467\n",
            "epoch 15 iter 380 val_loss: 1.495\n",
            "epoch 15 iter 400 val_loss: 1.452\n",
            "epoch 15 iter 420 val_loss: 1.455\n",
            "epoch 15 iter 440 val_loss: 1.46\n",
            "epoch 15 iter 460 val_loss: 1.46\n",
            "epoch 15 iter 480 val_loss: 1.471\n",
            "epoch 15 iter 500 val_loss: 1.479\n",
            "epoch 15 iter 520 val_loss: 1.488\n",
            "epoch 15 iter 540 val_loss: 1.505\n",
            "epoch 15 iter 560 val_loss: 1.502\n",
            "epoch 15 iter 580 val_loss: 1.495\n",
            "epoch 15 iter 600 val_loss: 1.459\n",
            "epoch 16 iter 0 val_loss: 1.487\n",
            "epoch 16 iter 20 val_loss: 1.466\n",
            "epoch 16 iter 40 val_loss: 1.475\n",
            "epoch 16 iter 60 val_loss: 1.468\n",
            "epoch 16 iter 80 val_loss: 1.484\n",
            "epoch 16 iter 100 val_loss: 1.464\n",
            "epoch 16 iter 120 val_loss: 1.447\n",
            "epoch 16 iter 140 val_loss: 1.464\n",
            "epoch 16 iter 160 val_loss: 1.447\n",
            "epoch 16 iter 180 val_loss: 1.478\n",
            "epoch 16 iter 200 val_loss: 1.458\n",
            "epoch 16 iter 220 val_loss: 1.455\n",
            "epoch 16 iter 240 val_loss: 1.447\n",
            "epoch 16 iter 260 val_loss: 1.475\n",
            "epoch 16 iter 280 val_loss: 1.466\n",
            "epoch 16 iter 300 val_loss: 1.427\n",
            "epoch 16 iter 320 val_loss: 1.488\n",
            "epoch 16 iter 340 val_loss: 1.46\n",
            "epoch 16 iter 360 val_loss: 1.472\n",
            "epoch 16 iter 380 val_loss: 1.447\n",
            "epoch 16 iter 400 val_loss: 1.468\n",
            "epoch 16 iter 420 val_loss: 1.466\n",
            "epoch 16 iter 440 val_loss: 1.482\n",
            "epoch 16 iter 460 val_loss: 1.494\n",
            "epoch 16 iter 480 val_loss: 1.479\n",
            "epoch 16 iter 500 val_loss: 1.492\n",
            "epoch 16 iter 520 val_loss: 1.474\n",
            "epoch 16 iter 540 val_loss: 1.424\n",
            "epoch 16 iter 560 val_loss: 1.441\n",
            "epoch 16 iter 580 val_loss: 1.431\n",
            "epoch 16 iter 600 val_loss: 1.454\n",
            "epoch 17 iter 0 val_loss: 1.46\n",
            "epoch 17 iter 20 val_loss: 1.458\n",
            "epoch 17 iter 40 val_loss: 1.444\n",
            "epoch 17 iter 60 val_loss: 1.44\n",
            "epoch 17 iter 80 val_loss: 1.445\n",
            "epoch 17 iter 100 val_loss: 1.479\n",
            "epoch 17 iter 120 val_loss: 1.471\n",
            "epoch 17 iter 140 val_loss: 1.438\n",
            "epoch 17 iter 160 val_loss: 1.457\n",
            "epoch 17 iter 180 val_loss: 1.486\n",
            "epoch 17 iter 200 val_loss: 1.488\n",
            "epoch 17 iter 220 val_loss: 1.481\n",
            "epoch 17 iter 240 val_loss: 1.468\n",
            "epoch 17 iter 260 val_loss: 1.479\n",
            "epoch 17 iter 280 val_loss: 1.479\n",
            "epoch 17 iter 300 val_loss: 1.469\n",
            "epoch 17 iter 320 val_loss: 1.46\n",
            "epoch 17 iter 340 val_loss: 1.43\n",
            "epoch 17 iter 360 val_loss: 1.458\n",
            "epoch 17 iter 380 val_loss: 1.461\n",
            "epoch 17 iter 400 val_loss: 1.453\n",
            "epoch 17 iter 420 val_loss: 1.406\n",
            "epoch 17 iter 440 val_loss: 1.426\n",
            "epoch 17 iter 460 val_loss: 1.472\n",
            "epoch 17 iter 480 val_loss: 1.443\n",
            "epoch 17 iter 500 val_loss: 1.442\n",
            "epoch 17 iter 520 val_loss: 1.451\n",
            "epoch 17 iter 540 val_loss: 1.468\n",
            "epoch 17 iter 560 val_loss: 1.495\n",
            "epoch 17 iter 580 val_loss: 1.493\n",
            "epoch 17 iter 600 val_loss: 1.482\n",
            "epoch 18 iter 0 val_loss: 1.452\n",
            "epoch 18 iter 20 val_loss: 1.425\n",
            "epoch 18 iter 40 val_loss: 1.439\n",
            "epoch 18 iter 60 val_loss: 1.426\n",
            "epoch 18 iter 80 val_loss: 1.429\n",
            "epoch 18 iter 100 val_loss: 1.495\n",
            "epoch 18 iter 120 val_loss: 1.455\n",
            "epoch 18 iter 140 val_loss: 1.463\n",
            "epoch 18 iter 160 val_loss: 1.493\n",
            "epoch 18 iter 180 val_loss: 1.447\n",
            "epoch 18 iter 200 val_loss: 1.459\n",
            "epoch 18 iter 220 val_loss: 1.47\n",
            "epoch 18 iter 240 val_loss: 1.517\n",
            "epoch 18 iter 260 val_loss: 1.51\n",
            "epoch 18 iter 280 val_loss: 1.506\n",
            "epoch 18 iter 300 val_loss: 1.488\n",
            "epoch 18 iter 320 val_loss: 1.464\n",
            "epoch 18 iter 340 val_loss: 1.469\n",
            "epoch 18 iter 360 val_loss: 1.461\n",
            "epoch 18 iter 380 val_loss: 1.457\n",
            "epoch 18 iter 400 val_loss: 1.433\n",
            "epoch 18 iter 420 val_loss: 1.427\n",
            "epoch 18 iter 440 val_loss: 1.467\n",
            "epoch 18 iter 460 val_loss: 1.476\n",
            "epoch 18 iter 480 val_loss: 1.436\n",
            "epoch 18 iter 500 val_loss: 1.421\n",
            "epoch 18 iter 520 val_loss: 1.489\n",
            "epoch 18 iter 540 val_loss: 1.437\n",
            "epoch 18 iter 560 val_loss: 1.459\n",
            "epoch 18 iter 580 val_loss: 1.404\n",
            "epoch 18 iter 600 val_loss: 1.414\n",
            "epoch 19 iter 0 val_loss: 1.429\n",
            "epoch 19 iter 20 val_loss: 1.455\n",
            "epoch 19 iter 40 val_loss: 1.444\n",
            "epoch 19 iter 60 val_loss: 1.455\n",
            "epoch 19 iter 80 val_loss: 1.453\n",
            "epoch 19 iter 100 val_loss: 1.451\n",
            "epoch 19 iter 120 val_loss: 1.419\n",
            "epoch 19 iter 140 val_loss: 1.463\n",
            "epoch 19 iter 160 val_loss: 1.425\n",
            "epoch 19 iter 180 val_loss: 1.428\n",
            "epoch 19 iter 200 val_loss: 1.425\n",
            "epoch 19 iter 220 val_loss: 1.437\n",
            "epoch 19 iter 240 val_loss: 1.462\n",
            "epoch 19 iter 260 val_loss: 1.451\n",
            "epoch 19 iter 280 val_loss: 1.442\n",
            "epoch 19 iter 300 val_loss: 1.434\n",
            "epoch 19 iter 320 val_loss: 1.436\n",
            "epoch 19 iter 340 val_loss: 1.394\n",
            "epoch 19 iter 360 val_loss: 1.382\n",
            "epoch 19 iter 380 val_loss: 1.378\n",
            "epoch 19 iter 400 val_loss: 1.426\n",
            "epoch 19 iter 420 val_loss: 1.445\n",
            "epoch 19 iter 440 val_loss: 1.418\n",
            "epoch 19 iter 460 val_loss: 1.402\n",
            "epoch 19 iter 480 val_loss: 1.445\n",
            "epoch 19 iter 500 val_loss: 1.401\n",
            "epoch 19 iter 520 val_loss: 1.435\n",
            "epoch 19 iter 540 val_loss: 1.421\n",
            "epoch 19 iter 560 val_loss: 1.427\n",
            "epoch 19 iter 580 val_loss: 1.414\n",
            "epoch 19 iter 600 val_loss: 1.392\n",
            "epoch 20 iter 0 val_loss: 1.417\n",
            "epoch 20 iter 20 val_loss: 1.424\n",
            "epoch 20 iter 40 val_loss: 1.419\n",
            "epoch 20 iter 60 val_loss: 1.397\n",
            "epoch 20 iter 80 val_loss: 1.382\n",
            "epoch 20 iter 100 val_loss: 1.388\n",
            "epoch 20 iter 120 val_loss: 1.433\n",
            "epoch 20 iter 140 val_loss: 1.418\n",
            "epoch 20 iter 160 val_loss: 1.431\n",
            "epoch 20 iter 180 val_loss: 1.393\n",
            "epoch 20 iter 200 val_loss: 1.441\n",
            "epoch 20 iter 220 val_loss: 1.424\n",
            "epoch 20 iter 240 val_loss: 1.418\n",
            "epoch 20 iter 260 val_loss: 1.454\n",
            "epoch 20 iter 280 val_loss: 1.417\n",
            "epoch 20 iter 300 val_loss: 1.449\n",
            "epoch 20 iter 320 val_loss: 1.453\n",
            "epoch 20 iter 340 val_loss: 1.408\n",
            "epoch 20 iter 360 val_loss: 1.46\n",
            "epoch 20 iter 380 val_loss: 1.409\n",
            "epoch 20 iter 400 val_loss: 1.443\n",
            "epoch 20 iter 420 val_loss: 1.434\n",
            "epoch 20 iter 440 val_loss: 1.405\n",
            "epoch 20 iter 460 val_loss: 1.383\n",
            "epoch 20 iter 480 val_loss: 1.396\n",
            "epoch 20 iter 500 val_loss: 1.392\n",
            "epoch 20 iter 520 val_loss: 1.398\n",
            "epoch 20 iter 540 val_loss: 1.398\n",
            "epoch 20 iter 560 val_loss: 1.418\n",
            "epoch 20 iter 580 val_loss: 1.377\n",
            "epoch 20 iter 600 val_loss: 1.373\n",
            "epoch 21 iter 0 val_loss: 1.358\n",
            "epoch 21 iter 20 val_loss: 1.39\n",
            "epoch 21 iter 40 val_loss: 1.379\n",
            "epoch 21 iter 60 val_loss: 1.375\n",
            "epoch 21 iter 80 val_loss: 1.342\n",
            "epoch 21 iter 100 val_loss: 1.334\n",
            "epoch 21 iter 120 val_loss: 1.352\n",
            "epoch 21 iter 140 val_loss: 1.351\n",
            "epoch 21 iter 160 val_loss: 1.338\n",
            "epoch 21 iter 180 val_loss: 1.388\n",
            "epoch 21 iter 200 val_loss: 1.39\n",
            "epoch 21 iter 220 val_loss: 1.448\n",
            "epoch 21 iter 240 val_loss: 1.421\n",
            "epoch 21 iter 260 val_loss: 1.382\n",
            "epoch 21 iter 280 val_loss: 1.371\n",
            "epoch 21 iter 300 val_loss: 1.353\n",
            "epoch 21 iter 320 val_loss: 1.361\n",
            "epoch 21 iter 340 val_loss: 1.4\n",
            "epoch 21 iter 360 val_loss: 1.451\n",
            "epoch 21 iter 380 val_loss: 1.483\n",
            "epoch 21 iter 400 val_loss: 1.464\n",
            "epoch 21 iter 420 val_loss: 1.388\n",
            "epoch 21 iter 440 val_loss: 1.396\n",
            "epoch 21 iter 460 val_loss: 1.414\n",
            "epoch 21 iter 480 val_loss: 1.416\n",
            "epoch 21 iter 500 val_loss: 1.358\n",
            "epoch 21 iter 520 val_loss: 1.346\n",
            "epoch 21 iter 540 val_loss: 1.357\n",
            "epoch 21 iter 560 val_loss: 1.367\n",
            "epoch 21 iter 580 val_loss: 1.404\n",
            "epoch 21 iter 600 val_loss: 1.412\n",
            "epoch 22 iter 0 val_loss: 1.421\n",
            "epoch 22 iter 20 val_loss: 1.398\n",
            "epoch 22 iter 40 val_loss: 1.388\n",
            "epoch 22 iter 60 val_loss: 1.407\n",
            "epoch 22 iter 80 val_loss: 1.383\n",
            "epoch 22 iter 100 val_loss: 1.396\n",
            "epoch 22 iter 120 val_loss: 1.389\n",
            "epoch 22 iter 140 val_loss: 1.374\n",
            "epoch 22 iter 160 val_loss: 1.398\n",
            "epoch 22 iter 180 val_loss: 1.396\n",
            "epoch 22 iter 200 val_loss: 1.414\n",
            "epoch 22 iter 220 val_loss: 1.392\n",
            "epoch 22 iter 240 val_loss: 1.364\n",
            "epoch 22 iter 260 val_loss: 1.332\n",
            "epoch 22 iter 280 val_loss: 1.345\n",
            "epoch 22 iter 300 val_loss: 1.364\n",
            "epoch 22 iter 320 val_loss: 1.364\n",
            "epoch 22 iter 340 val_loss: 1.362\n",
            "epoch 22 iter 360 val_loss: 1.43\n",
            "epoch 22 iter 380 val_loss: 1.396\n",
            "epoch 22 iter 400 val_loss: 1.391\n",
            "epoch 22 iter 420 val_loss: 1.388\n",
            "epoch 22 iter 440 val_loss: 1.352\n",
            "epoch 22 iter 460 val_loss: 1.352\n",
            "epoch 22 iter 480 val_loss: 1.372\n",
            "epoch 22 iter 500 val_loss: 1.346\n",
            "epoch 22 iter 520 val_loss: 1.346\n",
            "epoch 22 iter 540 val_loss: 1.359\n",
            "epoch 22 iter 560 val_loss: 1.336\n",
            "epoch 22 iter 580 val_loss: 1.358\n",
            "epoch 22 iter 600 val_loss: 1.337\n",
            "epoch 23 iter 0 val_loss: 1.356\n",
            "epoch 23 iter 20 val_loss: 1.368\n",
            "epoch 23 iter 40 val_loss: 1.365\n",
            "epoch 23 iter 60 val_loss: 1.36\n",
            "epoch 23 iter 80 val_loss: 1.368\n",
            "epoch 23 iter 100 val_loss: 1.361\n",
            "epoch 23 iter 120 val_loss: 1.367\n",
            "epoch 23 iter 140 val_loss: 1.333\n",
            "epoch 23 iter 160 val_loss: 1.393\n",
            "epoch 23 iter 180 val_loss: 1.396\n",
            "epoch 23 iter 200 val_loss: 1.361\n",
            "epoch 23 iter 220 val_loss: 1.376\n",
            "epoch 23 iter 240 val_loss: 1.388\n",
            "epoch 23 iter 260 val_loss: 1.373\n",
            "epoch 23 iter 280 val_loss: 1.335\n",
            "epoch 23 iter 300 val_loss: 1.325\n",
            "epoch 23 iter 320 val_loss: 1.366\n",
            "epoch 23 iter 340 val_loss: 1.363\n",
            "epoch 23 iter 360 val_loss: 1.385\n",
            "epoch 23 iter 380 val_loss: 1.355\n",
            "epoch 23 iter 400 val_loss: 1.379\n",
            "epoch 23 iter 420 val_loss: 1.396\n",
            "epoch 23 iter 440 val_loss: 1.375\n",
            "epoch 23 iter 460 val_loss: 1.36\n",
            "epoch 23 iter 480 val_loss: 1.362\n",
            "epoch 23 iter 500 val_loss: 1.378\n",
            "epoch 23 iter 520 val_loss: 1.358\n",
            "epoch 23 iter 540 val_loss: 1.433\n",
            "epoch 23 iter 560 val_loss: 1.395\n",
            "epoch 23 iter 580 val_loss: 1.352\n",
            "epoch 23 iter 600 val_loss: 1.403\n",
            "epoch 24 iter 0 val_loss: 1.356\n",
            "epoch 24 iter 20 val_loss: 1.376\n",
            "epoch 24 iter 40 val_loss: 1.376\n",
            "epoch 24 iter 60 val_loss: 1.355\n",
            "epoch 24 iter 80 val_loss: 1.363\n",
            "epoch 24 iter 100 val_loss: 1.35\n",
            "epoch 24 iter 120 val_loss: 1.32\n",
            "epoch 24 iter 140 val_loss: 1.381\n",
            "epoch 24 iter 160 val_loss: 1.413\n",
            "epoch 24 iter 180 val_loss: 1.342\n",
            "epoch 24 iter 200 val_loss: 1.371\n",
            "epoch 24 iter 220 val_loss: 1.368\n",
            "epoch 24 iter 240 val_loss: 1.374\n",
            "epoch 24 iter 260 val_loss: 1.331\n",
            "epoch 24 iter 280 val_loss: 1.305\n",
            "epoch 24 iter 300 val_loss: 1.36\n",
            "epoch 24 iter 320 val_loss: 1.36\n",
            "epoch 24 iter 340 val_loss: 1.39\n",
            "epoch 24 iter 360 val_loss: 1.372\n",
            "epoch 24 iter 380 val_loss: 1.363\n",
            "epoch 24 iter 400 val_loss: 1.377\n",
            "epoch 24 iter 420 val_loss: 1.368\n",
            "epoch 24 iter 440 val_loss: 1.367\n",
            "epoch 24 iter 460 val_loss: 1.393\n",
            "epoch 24 iter 480 val_loss: 1.413\n",
            "epoch 24 iter 500 val_loss: 1.432\n",
            "epoch 24 iter 520 val_loss: 1.394\n",
            "epoch 24 iter 540 val_loss: 1.397\n",
            "epoch 24 iter 560 val_loss: 1.417\n",
            "epoch 24 iter 580 val_loss: 1.373\n",
            "epoch 24 iter 600 val_loss: 1.363\n",
            "epoch 25 iter 0 val_loss: 1.362\n",
            "epoch 25 iter 20 val_loss: 1.384\n",
            "epoch 25 iter 40 val_loss: 1.436\n",
            "epoch 25 iter 60 val_loss: 1.386\n",
            "epoch 25 iter 80 val_loss: 1.388\n",
            "epoch 25 iter 100 val_loss: 1.376\n",
            "epoch 25 iter 120 val_loss: 1.354\n",
            "epoch 25 iter 140 val_loss: 1.368\n",
            "epoch 25 iter 160 val_loss: 1.386\n",
            "epoch 25 iter 180 val_loss: 1.379\n",
            "epoch 25 iter 200 val_loss: 1.425\n",
            "epoch 25 iter 220 val_loss: 1.382\n",
            "epoch 25 iter 240 val_loss: 1.376\n",
            "epoch 25 iter 260 val_loss: 1.363\n",
            "epoch 25 iter 280 val_loss: 1.33\n",
            "epoch 25 iter 300 val_loss: 1.351\n",
            "epoch 25 iter 320 val_loss: 1.384\n",
            "epoch 25 iter 340 val_loss: 1.362\n",
            "epoch 25 iter 360 val_loss: 1.342\n",
            "epoch 25 iter 380 val_loss: 1.357\n",
            "epoch 25 iter 400 val_loss: 1.392\n",
            "epoch 25 iter 420 val_loss: 1.416\n",
            "epoch 25 iter 440 val_loss: 1.392\n",
            "epoch 25 iter 460 val_loss: 1.321\n",
            "epoch 25 iter 480 val_loss: 1.359\n",
            "epoch 25 iter 500 val_loss: 1.374\n",
            "epoch 25 iter 520 val_loss: 1.414\n",
            "epoch 25 iter 540 val_loss: 1.406\n",
            "epoch 25 iter 560 val_loss: 1.395\n",
            "epoch 25 iter 580 val_loss: 1.385\n",
            "epoch 25 iter 600 val_loss: 1.376\n",
            "epoch 26 iter 0 val_loss: 1.353\n",
            "epoch 26 iter 20 val_loss: 1.374\n",
            "epoch 26 iter 40 val_loss: 1.418\n",
            "epoch 26 iter 60 val_loss: 1.388\n",
            "epoch 26 iter 80 val_loss: 1.39\n",
            "epoch 26 iter 100 val_loss: 1.355\n",
            "epoch 26 iter 120 val_loss: 1.35\n",
            "epoch 26 iter 140 val_loss: 1.342\n",
            "epoch 26 iter 160 val_loss: 1.368\n",
            "epoch 26 iter 180 val_loss: 1.341\n",
            "epoch 26 iter 200 val_loss: 1.335\n",
            "epoch 26 iter 220 val_loss: 1.402\n",
            "epoch 26 iter 240 val_loss: 1.407\n",
            "epoch 26 iter 260 val_loss: 1.403\n",
            "epoch 26 iter 280 val_loss: 1.401\n",
            "epoch 26 iter 300 val_loss: 1.413\n",
            "epoch 26 iter 320 val_loss: 1.434\n",
            "epoch 26 iter 340 val_loss: 1.372\n",
            "epoch 26 iter 360 val_loss: 1.347\n",
            "epoch 26 iter 380 val_loss: 1.353\n",
            "epoch 26 iter 400 val_loss: 1.316\n",
            "epoch 26 iter 420 val_loss: 1.375\n",
            "epoch 26 iter 440 val_loss: 1.367\n",
            "epoch 26 iter 460 val_loss: 1.396\n",
            "epoch 26 iter 480 val_loss: 1.359\n",
            "epoch 26 iter 500 val_loss: 1.376\n",
            "epoch 26 iter 520 val_loss: 1.323\n",
            "epoch 26 iter 540 val_loss: 1.326\n",
            "epoch 26 iter 560 val_loss: 1.364\n",
            "epoch 26 iter 580 val_loss: 1.346\n",
            "epoch 26 iter 600 val_loss: 1.334\n",
            "epoch 27 iter 0 val_loss: 1.316\n",
            "epoch 27 iter 20 val_loss: 1.341\n",
            "epoch 27 iter 40 val_loss: 1.334\n",
            "epoch 27 iter 60 val_loss: 1.329\n",
            "epoch 27 iter 80 val_loss: 1.402\n",
            "epoch 27 iter 100 val_loss: 1.343\n",
            "epoch 27 iter 120 val_loss: 1.369\n",
            "epoch 27 iter 140 val_loss: 1.379\n",
            "epoch 27 iter 160 val_loss: 1.438\n",
            "epoch 27 iter 180 val_loss: 1.392\n",
            "epoch 27 iter 200 val_loss: 1.371\n",
            "epoch 27 iter 220 val_loss: 1.361\n",
            "epoch 27 iter 240 val_loss: 1.361\n",
            "epoch 27 iter 260 val_loss: 1.347\n",
            "epoch 27 iter 280 val_loss: 1.368\n",
            "epoch 27 iter 300 val_loss: 1.381\n",
            "epoch 27 iter 320 val_loss: 1.327\n",
            "epoch 27 iter 340 val_loss: 1.331\n",
            "epoch 27 iter 360 val_loss: 1.371\n",
            "epoch 27 iter 380 val_loss: 1.354\n",
            "epoch 27 iter 400 val_loss: 1.388\n",
            "epoch 27 iter 420 val_loss: 1.369\n",
            "epoch 27 iter 440 val_loss: 1.389\n",
            "epoch 27 iter 460 val_loss: 1.343\n",
            "epoch 27 iter 480 val_loss: 1.387\n",
            "epoch 27 iter 500 val_loss: 1.362\n",
            "epoch 27 iter 520 val_loss: 1.321\n",
            "epoch 27 iter 540 val_loss: 1.345\n",
            "epoch 27 iter 560 val_loss: 1.329\n",
            "epoch 27 iter 580 val_loss: 1.315\n",
            "epoch 27 iter 600 val_loss: 1.367\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSzvBQM19b8W"
      },
      "source": [
        "## Генерация команд (2 балла)\n",
        "\n",
        "**Задание**. Реализуйте алгоритм beam-search в классе BeamSearchGenerator ниже. Ваша реализация должна поддерживать задание температуры софтмакса. Выходы модели, полученные на предыдущих итерациях, необходимо кэшировать для повышения скорости алгоритма. Вместо подсчёта произведения любых вероятностей необходимо считать сумму их логарифмов.\n",
        "\n",
        "Алгоритм должен возвращать список пар из получившихся выходных последовательностей и логарифмов их вероятностей. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "v-ioGJw39b8W"
      },
      "outputs": [],
      "source": [
        "class BeamSearchGenerator:\n",
        "    def __init__(\n",
        "            self, pad_id, eos_id, bos_id,\n",
        "            max_length=20, beam_width=5, temperature=1,\n",
        "            device='cuda',\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        pad_id : int\n",
        "        eos_id : int\n",
        "        bos_id : int\n",
        "        max_length : int\n",
        "            Maximum length of output sequence\n",
        "        beam_width : int\n",
        "            Width of the beam\n",
        "        temperature : float\n",
        "            Softmax temperature\n",
        "        device : torch.device\n",
        "            Your model device\n",
        "        \"\"\"\n",
        "        self.pad_id = pad_id\n",
        "        self.eos_id = eos_id\n",
        "        self.bos_id = bos_id\n",
        "        \n",
        "        self.max_length = max_length\n",
        "        self.beam_width = beam_width\n",
        "        self.temperature = temperature\n",
        "        \n",
        "        self.device = device\n",
        "        \n",
        "    def get_result(self, model, input_text_tokens):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        model : TextToBashModel\n",
        "        input_text_tokens : torch.tensor\n",
        "            One object input tensor\n",
        "        \"\"\"\n",
        "        ## YOUR CODE HERE ##\n",
        "        if len(input_text_tokens.shape) == 1:\n",
        "            input_text_tokens = input_text_tokens.unsqueeze(0)\n",
        "        model = model.to(self.device)\n",
        "        input_text_tokens = input_text_tokens.to(self.device)\n",
        "        beam_output = model._coupling.generate(input_text_tokens,\n",
        "                                               max_length=self.max_length,\n",
        "                                               num_return_sequences=self.beam_width,\n",
        "                                               output_scores=True,\n",
        "                                               return_dict_in_generate=True,\n",
        "                                               num_beams=self.beam_width,\n",
        "                                               bos_token_id=self.bos_id,\n",
        "                                               pad_token_id=self.pad_id,\n",
        "                                               eos_token_id=self.eos_id,\n",
        "                                               temperature=self.temperature)\n",
        "        output = list(zip(beam_output.sequences.cpu().numpy(), beam_output.sequences_scores.cpu().numpy()))\n",
        "        return [(seq[seq != self.pad_id], prob) for seq, prob in output]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zgYCJAN9b8X"
      },
      "source": [
        "Протестируйте на нескольких примерах работу вашего алгоритма. Если всё реализовано правильно, то как минимум на трёх примерах из 5 всё должно работать правильно."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "sU1habAW9b8X"
      },
      "outputs": [],
      "source": [
        "beam_search_engine = BeamSearchGenerator(\n",
        "    pad_id=PAD_ID, eos_id=EOS_ID, bos_id=BOS_ID,\n",
        "    max_length=MAX_CODE_LENGTH, beam_width=5,\n",
        "    temperature=0.6, device='cuda',\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "UF1DrTUv9b8X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76c8cb6c-1df5-4df1-ab59-10ef6f2bd8c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "text: searches through the root filesystem (\"/\") for the file named chapter1, and prints the location\n",
            "true: find / -name Chapter1 -type f -print\n",
            "true cleaned: find Path -name Regex -type f -print\n",
            "find Path -name Regex -type f -print -0.15314859\n",
            "find Path -name Regex -type f -0.18424827\n",
            "find Path -name Regex -type f -print0 -0.2090472\n",
            "find Path -name Regex -prune -or -name Regex -print -0.25227\n",
            "find Path -name Regex -type f -exec echo Regex {} \\; -0.25723422\n",
            "1.0\n",
            "\n",
            "text: searches through the root filesystem (\"/\") for the file named chapter1.\n",
            "true: find / -name Chapter1 -type f\n",
            "true cleaned: find Path -name Regex -type f\n",
            "find Path -name Regex -print -0.192784\n",
            "find Path -name Regex -prune -or -name Regex -print -0.21505143\n",
            "find Path -name Regex -type f -print -0.24424024\n",
            "find Path -name Regex -or -name Regex -0.26890096\n",
            "find Path -type f -name Regex -print -0.2783678\n",
            "0.6666666666666666\n",
            "\n",
            "text: searches through the root filesystem (\"/\") for the file named chapter1.\n",
            "true: find / -name Chapter1 -type f -print\n",
            "true cleaned: find Path -name Regex -type f -print\n",
            "find Path -name Regex -print -0.12471461\n",
            "find Path -name Regex -exec grep -l Regex {} \\; -0.26183993\n",
            "find Path -name Regex -type f -0.2740819\n",
            "find Path -name Regex -prune -or -name Regex -print -0.2804408\n",
            "find Path -name Regex -exec grep -i Regex {} \\; -0.2966624\n",
            "0.6666666666666666\n",
            "\n",
            "text: searching for all files with the extension mp3\n",
            "true: find / -name *.mp3\n",
            "true cleaned: find Path -name Regex\n",
            "find Path -type f -name Regex -0.124267645\n",
            "find Path -type f -name Regex -print -0.2070837\n",
            "find Path -type f -name Regex -exec grep -l Regex {} \\; -0.22276141\n",
            "find Path -type f -name Regex -exec ls -l {} \\; -0.22361961\n",
            "find Path -type f -name Regex -exec grep Regex {} \\; -0.2658588\n",
            "0.5\n",
            "\n",
            "text: set myvariable to the value of variable_name\n",
            "true: myVariable=$(env  | grep VARIABLE_NAME | grep -oe '[^=]*$');\n",
            "true cleaned: env | grep Regex | grep -o -e Regex\n",
            "env Program Program | grep -o Regex -0.6333137\n",
            "set Regex | grep -o Regex -0.64702255\n",
            "env Program Program | grep Regex Program -0.8485036\n",
            "env -i Program Program Program -0.8782029\n",
            "env -i Program Program Program Program -0.8814052\n",
            "0.3333333333333333\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    for i in range(5):\n",
        "        print()\n",
        "        print('text:', valid_data.invocation.iloc[i])\n",
        "        print('true:', valid_data.cmd.iloc[i])\n",
        "        print('true cleaned:', valid_data.cmd_cleaned.iloc[i])\n",
        "\n",
        "        src = torch.concat(valid_ds[i], dim=0)\n",
        "        pred = beam_search_engine.get_result(model, src)\n",
        "        \n",
        "        scores = []\n",
        "        for x, proba in pred:\n",
        "            pred_cmd = cmd_tokenizer.decode(list(map(int, x)))\n",
        "            score = compute_metric(pred_cmd, 1, valid_data.cmd.iloc[i])\n",
        "            scores.append(score)\n",
        "            print(pred_cmd, proba)\n",
        "        print(max(scores))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBM6Nakt9b8Y"
      },
      "source": [
        "**Задание**. Дополните функцию для подсчёта качества. Посчитайте качество вашей модели на валидационном и тестовых датасетов."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "TPOxk1Jg9b8Y"
      },
      "outputs": [],
      "source": [
        "def compute_all_scores(model, df, beam_engine):\n",
        "    all_scores = []\n",
        "\n",
        "    for i, (text, target_cmd) in enumerate(zip(df.text_cleaned.values, df.cmd.values)):\n",
        "        input_tokens = torch.LongTensor(text_tokenizer.Encode(' '.join(text), add_bos=True, add_eos=True)) ## YOUR CODE HERE ##\n",
        "        predictions = beam_engine.get_result(model, input_tokens)\n",
        "        \n",
        "        # get only 5 top results\n",
        "        predictions = predictions[:5]\n",
        "        object_scores = []\n",
        "        for output_tokens, proba in predictions:\n",
        "            output_cmd = cmd_tokenizer.decode(list(map(int, output_tokens)))\n",
        "            score = compute_metric(output_cmd, 1, target_cmd)\n",
        "            object_scores.append(score)\n",
        "        \n",
        "        all_scores.append(max(object_scores))\n",
        "    return all_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJoj-xtI9b8Y"
      },
      "source": [
        "Ваша цель при помощи подбора параметров модели и генерации получить средний скор на валидации >= 0.2, скор `handcrafted` части теста >= 0.1. На `mined` части датасета скор может быть низкий, т.к. некоторых команд из датасета нет в обучении.\n",
        "\n",
        "**Обратите внимание.** Так как датасет для обучения не очень большой, а данные достаточно нестабильные, подбор параметров может очень сильно влиять на модель. Некоторые полезные советы:\n",
        "* Отслеживайте качество модели после каждой эпохи, не забывайте про early stopping\n",
        "* Вы можете сразу приступить к следующей части. Побитие скора в этой части задания при помощи трюков из бонусной части считается валидным."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "5jlKXLZO9b8Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c101c4e-94be-4560-c4e9-39252107b687"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "valid 0.19933333333333333\n",
            "handcrafted 0.09190353143841516\n",
            "mined -0.4139713374088374\n"
          ]
        }
      ],
      "source": [
        "## YOUR CODE HERE ##\n",
        "valid = np.mean(compute_all_scores(model, valid_data, beam_search_engine))\n",
        "handcrafted = np.mean(compute_all_scores(model, test_data[test_data['origin'] == 'handcrafted'], beam_search_engine))\n",
        "mined = np.mean(compute_all_scores(model, test_data[test_data['origin'] == 'mined'], beam_search_engine))\n",
        "print(f'valid {valid}')\n",
        "print(f'handcrafted {handcrafted}')\n",
        "print(f'mined {mined}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HH2rtR79b8Y"
      },
      "source": [
        "## Улучшение модели (4 балла)\n",
        "\n",
        "Вы реализовали бейзлайн, пришло время улучшить качество модели. Т.к. это последнее задание, мы не будем предлагать конкретные шаги, а только дадим несколько советов.\n",
        "\n",
        "1. Большой источник информации о работе командной строке — её документация, man. Один из способов улучшения модели - использование мана для генерации новых примеров. Структурированный ман можно найти по ссылке https://github.com/IBM/clai/blob/nlc2cmd/docs/manpage-data.md.\n",
        "2. Ещё один способ улучшить модель, разделить предсказание утилит и флагов. Т.к. задача предсказания утилит более важная, вы можете натренировать модель, которая предсказывает последовательность утилит, а затем к каждой утилите генерировать флаги.\n",
        "3. Можно аугментировать данные, чтобы увеличить выборку.\n",
        "4. Можно в качество входа подавать не только текстовый запрос, но и описание из мана. Т.к. всё описание достаточно большое, нужно сделать дополнительную модель, которая будет выбирать команды, для которых нужно вытащить описание.\n",
        "5. Найти дополнительные данные, улучшающие обучение\n",
        "6. Как всегда можно просто сделать больше слоёв, увеличить размер скрытого слоя и т.д."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_rBsXcA9b8Z"
      },
      "source": [
        "От вас ожидается скор на валидации >= 0.25, `mined` >= 0, `handrafted` >= 0.15."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3NRlDKn69b8Z"
      },
      "outputs": [],
      "source": [
        "## YOUR CODE HERE ##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGJbzm8v9b8Z"
      },
      "source": [
        "## Бонусные баллы (до 3 баллов)\n",
        "\n",
        "При существенном улучшении качества будут назначаться бонусные баллы. На тестовых датасетах реально выбить качество >= 0.3 на каждом, но усилий потребуется немало..."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}